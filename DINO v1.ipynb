{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1RRXxuRUWU1E"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import shutil\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.datasets import CocoDetection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import datetime\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "from os import path\n",
        "import os, sys\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, DistributedSampler\n",
        "import torch.distributed as dist\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "QJyLxfjYKdfS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UgTXkuQkKl35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/Pedestrian_dataset_for_internship_assignment.zip"
      ],
      "metadata": {
        "id": "ozh1_cxLW5Xc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/DINO\n",
        "!pip install -r /content/DINO/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l9aT_ICDJ3W3",
        "outputId": "7fb78697-e7c5-41fb-d942-3ebba2058313"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycocotools (from -r /content/DINO/requirements.txt (line 2))\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-install-4f0dx0hz/pycocotools_d5a79ddd65674fc3b1ea55b074f6f58f\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/cocoapi.git /tmp/pip-install-4f0dx0hz/pycocotools_d5a79ddd65674fc3b1ea55b074f6f58f\n",
            "  Resolved https://github.com/cocodataset/cocoapi.git to commit 8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting panopticapi (from -r /content/DINO/requirements.txt (line 6))\n",
            "  Cloning https://github.com/cocodataset/panopticapi.git to /tmp/pip-install-4f0dx0hz/panopticapi_e5ffbf976c184e45b967335053085daf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/panopticapi.git /tmp/pip-install-4f0dx0hz/panopticapi_e5ffbf976c184e45b967335053085daf\n",
            "  Resolved https://github.com/cocodataset/panopticapi.git to commit 7bb4655548f98f3fedc07bf37e9040a992b054b0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from -r /content/DINO/requirements.txt (line 1)) (3.0.11)\n",
            "Collecting submitit (from -r /content/DINO/requirements.txt (line 3))\n",
            "  Downloading submitit-1.5.2-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/DINO/requirements.txt (line 4)) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/DINO/requirements.txt (line 5)) (0.20.0+cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r /content/DINO/requirements.txt (line 7)) (1.13.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from -r /content/DINO/requirements.txt (line 8)) (2.5.0)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from -r /content/DINO/requirements.txt (line 9)) (2.4.0)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (from -r /content/DINO/requirements.txt (line 10)) (0.40.2)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from -r /content/DINO/requirements.txt (line 11)) (1.0.11)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools->-r /content/DINO/requirements.txt (line 2)) (75.1.0)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools->-r /content/DINO/requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from submitit->-r /content/DINO/requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.10/dist-packages (from submitit->-r /content/DINO/requirements.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r /content/DINO/requirements.txt (line 4)) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r /content/DINO/requirements.txt (line 4)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r /content/DINO/requirements.txt (line 4)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r /content/DINO/requirements.txt (line 4)) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r /content/DINO/requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.5.0->-r /content/DINO/requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.6.0->-r /content/DINO/requirements.txt (line 5)) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.6.0->-r /content/DINO/requirements.txt (line 5)) (10.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->-r /content/DINO/requirements.txt (line 10)) (8.5.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->-r /content/DINO/requirements.txt (line 10)) (4.3.6)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->-r /content/DINO/requirements.txt (line 10)) (2.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->-r /content/DINO/requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm->-r /content/DINO/requirements.txt (line 11)) (0.24.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->-r /content/DINO/requirements.txt (line 11)) (0.4.5)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->-r /content/DINO/requirements.txt (line 10)) (3.20.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r /content/DINO/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r /content/DINO/requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r /content/DINO/requirements.txt (line 2)) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r /content/DINO/requirements.txt (line 2)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r /content/DINO/requirements.txt (line 2)) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r /content/DINO/requirements.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r /content/DINO/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r /content/DINO/requirements.txt (line 11)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r /content/DINO/requirements.txt (line 11)) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.5.0->-r /content/DINO/requirements.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools->-r /content/DINO/requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r /content/DINO/requirements.txt (line 11)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r /content/DINO/requirements.txt (line 11)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r /content/DINO/requirements.txt (line 11)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r /content/DINO/requirements.txt (line 11)) (2024.8.30)\n",
            "Downloading submitit-1.5.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.9/74.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pycocotools, panopticapi\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp310-cp310-linux_x86_64.whl size=376087 sha256=93a4316ac57f2fc807b62f23bfa334980a8b112949b8f654ffd50abd04f9923f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-864u_nzt/wheels/39/61/b4/480fbddb4d3d6bc34083e7397bc6f5d1381f79acc68e9f3511\n",
            "  Building wheel for panopticapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for panopticapi: filename=panopticapi-0.1-py3-none-any.whl size=8258 sha256=54e6e16f034ce1327372d56d584c74ce056f0e0bb47ff26426c269f4fa50e268\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-864u_nzt/wheels/70/87/ae/5c2b138c967549070e3fe35f3b5fcaf1ed56e9f5483a09ee65\n",
            "Successfully built pycocotools panopticapi\n",
            "Installing collected packages: submitit, panopticapi, pycocotools\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.8\n",
            "    Uninstalling pycocotools-2.0.8:\n",
            "      Successfully uninstalled pycocotools-2.0.8\n",
            "Successfully installed panopticapi-0.1 pycocotools-2.0 submitit-1.5.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pycocotools"
                ]
              },
              "id": "4fd51a9679e444c4aa9af985c0846dd7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
        "import functools\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from termcolor import colored\n",
        "\n",
        "\n",
        "class _ColorfulFormatter(logging.Formatter):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self._root_name = kwargs.pop(\"root_name\") + \".\"\n",
        "        self._abbrev_name = kwargs.pop(\"abbrev_name\", \"\")\n",
        "        if len(self._abbrev_name):\n",
        "            self._abbrev_name = self._abbrev_name + \".\"\n",
        "        super(_ColorfulFormatter, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def formatMessage(self, record):\n",
        "        record.name = record.name.replace(self._root_name, self._abbrev_name)\n",
        "        log = super(_ColorfulFormatter, self).formatMessage(record)\n",
        "        if record.levelno == logging.WARNING:\n",
        "            prefix = colored(\"WARNING\", \"red\", attrs=[\"blink\"])\n",
        "        elif record.levelno == logging.ERROR or record.levelno == logging.CRITICAL:\n",
        "            prefix = colored(\"ERROR\", \"red\", attrs=[\"blink\", \"underline\"])\n",
        "        else:\n",
        "            return log\n",
        "        return prefix + \" \" + log\n",
        "\n",
        "\n",
        "# so that calling setup_logger multiple times won't add many handlers\n",
        "@functools.lru_cache()\n",
        "def setup_logger(\n",
        "    output=None, distributed_rank=0, *, color=True, name=\"imagenet\", abbrev_name=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Initialize the detectron2 logger and set its verbosity level to \"INFO\".\n",
        "\n",
        "    Args:\n",
        "        output (str): a file name or a directory to save log. If None, will not save log file.\n",
        "            If ends with \".txt\" or \".log\", assumed to be a file name.\n",
        "            Otherwise, logs will be saved to `output/log.txt`.\n",
        "        name (str): the root module name of this logger\n",
        "\n",
        "    Returns:\n",
        "        logging.Logger: a logger\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    logger.propagate = False\n",
        "\n",
        "    if abbrev_name is None:\n",
        "        abbrev_name = name\n",
        "\n",
        "    plain_formatter = logging.Formatter(\n",
        "        '[%(asctime)s.%(msecs)03d]: %(message)s',\n",
        "        datefmt='%m/%d %H:%M:%S'\n",
        "    )\n",
        "    # stdout logging: master only\n",
        "    if distributed_rank == 0:\n",
        "        ch = logging.StreamHandler(stream=sys.stdout)\n",
        "        ch.setLevel(logging.DEBUG)\n",
        "        if color:\n",
        "            formatter = _ColorfulFormatter(\n",
        "                colored(\"[%(asctime)s.%(msecs)03d]: \", \"green\") + \"%(message)s\",\n",
        "                datefmt=\"%m/%d %H:%M:%S\",\n",
        "                root_name=name,\n",
        "                abbrev_name=str(abbrev_name),\n",
        "            )\n",
        "        else:\n",
        "            formatter = plain_formatter\n",
        "        ch.setFormatter(formatter)\n",
        "        logger.addHandler(ch)\n",
        "\n",
        "    # file logging: all workers\n",
        "    if output is not None:\n",
        "        if output.endswith(\".txt\") or output.endswith(\".log\"):\n",
        "            filename = output\n",
        "        else:\n",
        "            filename = os.path.join(output, \"log.txt\")\n",
        "        if distributed_rank > 0:\n",
        "            filename = filename + f\".rank{distributed_rank}\"\n",
        "        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "        fh = logging.StreamHandler(_cached_log_stream(filename))\n",
        "        fh.setLevel(logging.DEBUG)\n",
        "        fh.setFormatter(plain_formatter)\n",
        "        logger.addHandler(fh)\n",
        "\n",
        "    return logger\n",
        "\n",
        "\n",
        "# cache the opened file object, so that different calls to `setup_logger`\n",
        "# with the same file name can safely write to the same file.\n",
        "@functools.lru_cache(maxsize=None)\n",
        "def _cached_log_stream(filename):\n",
        "    return open(filename, \"a\")\n"
      ],
      "metadata": {
        "id": "0dfFtrv7KnOq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
        "\"\"\"\n",
        "Misc functions, including distributed helpers.\n",
        "\n",
        "Mostly copy-paste from torchvision references.\n",
        "\"\"\"\n",
        "import os\n",
        "import random\n",
        "import subprocess\n",
        "import time\n",
        "from collections import OrderedDict, defaultdict, deque\n",
        "import datetime\n",
        "import pickle\n",
        "from typing import Optional, List\n",
        "\n",
        "import json, time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch import Tensor\n",
        "\n",
        "import colorsys\n",
        "\n",
        "# needed due to empty tensor bug in pytorch and torchvision 0.5\n",
        "import torchvision\n",
        "__torchvision_need_compat_flag = float(torchvision.__version__.split('.')[1]) < 7\n",
        "if __torchvision_need_compat_flag:\n",
        "    from torchvision.ops import _new_empty_tensor\n",
        "    from torchvision.ops.misc import _output_size\n",
        "\n",
        "\n",
        "class SmoothedValue(object):\n",
        "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
        "    window or the global series average.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size=20, fmt=None):\n",
        "        if fmt is None:\n",
        "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
        "        self.deque = deque(maxlen=window_size)\n",
        "        self.total = 0.0\n",
        "        self.count = 0\n",
        "        self.fmt = fmt\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self.deque.append(value)\n",
        "        self.count += n\n",
        "        self.total += value * n\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        \"\"\"\n",
        "        Warning: does not synchronize the deque!\n",
        "        \"\"\"\n",
        "        if not is_dist_avail_and_initialized():\n",
        "            return\n",
        "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(t)\n",
        "        t = t.tolist()\n",
        "        self.count = int(t[0])\n",
        "        self.total = t[1]\n",
        "\n",
        "    @property\n",
        "    def median(self):\n",
        "        d = torch.tensor(list(self.deque))\n",
        "        if d.shape[0] == 0:\n",
        "            return 0\n",
        "        return d.median().item()\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
        "        return d.mean().item()\n",
        "\n",
        "    @property\n",
        "    def global_avg(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    @property\n",
        "    def max(self):\n",
        "        return max(self.deque)\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.deque[-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fmt.format(\n",
        "            median=self.median,\n",
        "            avg=self.avg,\n",
        "            global_avg=self.global_avg,\n",
        "            max=self.max,\n",
        "            value=self.value)\n",
        "\n",
        "\n",
        "def all_gather(data):\n",
        "    \"\"\"\n",
        "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
        "    Args:\n",
        "        data: any picklable object\n",
        "    Returns:\n",
        "        list[data]: list of data gathered from each rank\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size == 1:\n",
        "        return [data]\n",
        "\n",
        "    # serialized to a Tensor\n",
        "    buffer = pickle.dumps(data)\n",
        "    storage = torch.ByteStorage.from_buffer(buffer)\n",
        "    tensor = torch.ByteTensor(storage).to(\"cuda\")\n",
        "\n",
        "    # obtain Tensor size of each rank\n",
        "    local_size = torch.tensor([tensor.numel()], device=\"cuda\")\n",
        "    size_list = [torch.tensor([0], device=\"cuda\") for _ in range(world_size)]\n",
        "    dist.all_gather(size_list, local_size)\n",
        "    size_list = [int(size.item()) for size in size_list]\n",
        "    max_size = max(size_list)\n",
        "\n",
        "    # receiving Tensor from all ranks\n",
        "    # we pad the tensor because torch all_gather does not support\n",
        "    # gathering tensors of different shapes\n",
        "    tensor_list = []\n",
        "    for _ in size_list:\n",
        "        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=\"cuda\"))\n",
        "    if local_size != max_size:\n",
        "        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=\"cuda\")\n",
        "        tensor = torch.cat((tensor, padding), dim=0)\n",
        "    dist.all_gather(tensor_list, tensor)\n",
        "\n",
        "    data_list = []\n",
        "    for size, tensor in zip(size_list, tensor_list):\n",
        "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
        "        data_list.append(pickle.loads(buffer))\n",
        "\n",
        "    return data_list\n",
        "\n",
        "\n",
        "def reduce_dict(input_dict, average=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_dict (dict): all the values will be reduced\n",
        "        average (bool): whether to do average or sum\n",
        "    Reduce the values in the dictionary from all processes so that all processes\n",
        "    have the averaged results. Returns a dict with the same fields as\n",
        "    input_dict, after reduction.\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size < 2:\n",
        "        return input_dict\n",
        "    with torch.no_grad():\n",
        "        names = []\n",
        "        values = []\n",
        "        # sort the keys so that they are consistent across processes\n",
        "        for k in sorted(input_dict.keys()):\n",
        "            names.append(k)\n",
        "            values.append(input_dict[k])\n",
        "        values = torch.stack(values, dim=0)\n",
        "        dist.all_reduce(values)\n",
        "        if average:\n",
        "            values /= world_size\n",
        "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
        "    return reduced_dict\n",
        "\n",
        "\n",
        "class MetricLogger(object):\n",
        "    def __init__(self, delimiter=\"\\t\"):\n",
        "        self.meters = defaultdict(SmoothedValue)\n",
        "        self.delimiter = delimiter\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                v = v.item()\n",
        "            assert isinstance(v, (float, int))\n",
        "            self.meters[k].update(v)\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        if attr in self.meters:\n",
        "            return self.meters[attr]\n",
        "        if attr in self.__dict__:\n",
        "            return self.__dict__[attr]\n",
        "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
        "            type(self).__name__, attr))\n",
        "\n",
        "    def __str__(self):\n",
        "        loss_str = []\n",
        "        for name, meter in self.meters.items():\n",
        "            # print(name, str(meter))\n",
        "            # import ipdb;ipdb.set_trace()\n",
        "            if meter.count > 0:\n",
        "                loss_str.append(\n",
        "                    \"{}: {}\".format(name, str(meter))\n",
        "                )\n",
        "        return self.delimiter.join(loss_str)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for meter in self.meters.values():\n",
        "            meter.synchronize_between_processes()\n",
        "\n",
        "    def add_meter(self, name, meter):\n",
        "        self.meters[name] = meter\n",
        "\n",
        "    def log_every(self, iterable, print_freq, header=None, logger=None):\n",
        "        if logger is None:\n",
        "            print_func = print\n",
        "        else:\n",
        "            print_func = logger.info\n",
        "\n",
        "        i = 0\n",
        "        if not header:\n",
        "            header = ''\n",
        "        start_time = time.time()\n",
        "        end = time.time()\n",
        "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
        "        if torch.cuda.is_available():\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}',\n",
        "                'max mem: {memory:.0f}'\n",
        "            ])\n",
        "        else:\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}'\n",
        "            ])\n",
        "        MB = 1024.0 * 1024.0\n",
        "        for obj in iterable:\n",
        "            data_time.update(time.time() - end)\n",
        "            yield obj\n",
        "\n",
        "            iter_time.update(time.time() - end)\n",
        "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
        "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                if torch.cuda.is_available():\n",
        "                    print_func(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time),\n",
        "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
        "                else:\n",
        "                    print_func(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time)))\n",
        "            i += 1\n",
        "            end = time.time()\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "        print_func('{} Total time: {} ({:.4f} s / it)'.format(\n",
        "            header, total_time_str, total_time / len(iterable)))\n",
        "\n",
        "\n",
        "def get_sha():\n",
        "    cwd = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "    def _run(command):\n",
        "        return subprocess.check_output(command, cwd=cwd).decode('ascii').strip()\n",
        "    sha = 'N/A'\n",
        "    diff = \"clean\"\n",
        "    branch = 'N/A'\n",
        "    try:\n",
        "        sha = _run(['git', 'rev-parse', 'HEAD'])\n",
        "        subprocess.check_output(['git', 'diff'], cwd=cwd)\n",
        "        diff = _run(['git', 'diff-index', 'HEAD'])\n",
        "        diff = \"has uncommited changes\" if diff else \"clean\"\n",
        "        branch = _run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])\n",
        "    except Exception:\n",
        "        pass\n",
        "    message = f\"sha: {sha}, status: {diff}, branch: {branch}\"\n",
        "    return message\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "\n",
        "    batch = list(zip(*batch))\n",
        "    batch[0] = nested_tensor_from_tensor_list(batch[0])\n",
        "    return tuple(batch)\n",
        "\n",
        "\n",
        "def _max_by_axis(the_list):\n",
        "    # type: (List[List[int]]) -> List[int]\n",
        "    maxes = the_list[0]\n",
        "    for sublist in the_list[1:]:\n",
        "        for index, item in enumerate(sublist):\n",
        "            maxes[index] = max(maxes[index], item)\n",
        "    return maxes\n",
        "\n",
        "\n",
        "class NestedTensor(object):\n",
        "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
        "        self.tensors = tensors\n",
        "        self.mask = mask\n",
        "        if mask == 'auto':\n",
        "            self.mask = torch.zeros_like(tensors).to(tensors.device)\n",
        "            if self.mask.dim() == 3:\n",
        "                self.mask = self.mask.sum(0).to(bool)\n",
        "            elif self.mask.dim() == 4:\n",
        "                self.mask = self.mask.sum(1).to(bool)\n",
        "            else:\n",
        "                raise ValueError(\"tensors dim must be 3 or 4 but {}({})\".format(self.tensors.dim(), self.tensors.shape))\n",
        "\n",
        "    def imgsize(self):\n",
        "        res = []\n",
        "        for i in range(self.tensors.shape[0]):\n",
        "            mask = self.mask[i]\n",
        "            maxH = (~mask).sum(0).max()\n",
        "            maxW = (~mask).sum(1).max()\n",
        "            res.append(torch.Tensor([maxH, maxW]))\n",
        "        return res\n",
        "\n",
        "    def to(self, device):\n",
        "        # type: (Device) -> NestedTensor # noqa\n",
        "        cast_tensor = self.tensors.to(device)\n",
        "        mask = self.mask\n",
        "        if mask is not None:\n",
        "            assert mask is not None\n",
        "            cast_mask = mask.to(device)\n",
        "        else:\n",
        "            cast_mask = None\n",
        "        return NestedTensor(cast_tensor, cast_mask)\n",
        "\n",
        "    def to_img_list_single(self, tensor, mask):\n",
        "        assert tensor.dim() == 3, \"dim of tensor should be 3 but {}\".format(tensor.dim())\n",
        "        maxH = (~mask).sum(0).max()\n",
        "        maxW = (~mask).sum(1).max()\n",
        "        img = tensor[:, :maxH, :maxW]\n",
        "        return img\n",
        "\n",
        "    def to_img_list(self):\n",
        "        \"\"\"remove the padding and convert to img list\n",
        "\n",
        "        Returns:\n",
        "            [type]: [description]\n",
        "        \"\"\"\n",
        "        if self.tensors.dim() == 3:\n",
        "            return self.to_img_list_single(self.tensors, self.mask)\n",
        "        else:\n",
        "            res = []\n",
        "            for i in range(self.tensors.shape[0]):\n",
        "                tensor_i = self.tensors[i]\n",
        "                mask_i = self.mask[i]\n",
        "                res.append(self.to_img_list_single(tensor_i, mask_i))\n",
        "            return res\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return self.tensors.device\n",
        "\n",
        "    def decompose(self):\n",
        "        return self.tensors, self.mask\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.tensors)\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return {\n",
        "            'tensors.shape': self.tensors.shape,\n",
        "            'mask.shape': self.mask.shape\n",
        "        }\n",
        "\n",
        "\n",
        "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
        "    # TODO make this more general\n",
        "    if tensor_list[0].ndim == 3:\n",
        "        if torchvision._is_tracing():\n",
        "            # nested_tensor_from_tensor_list() does not export well to ONNX\n",
        "            # call _onnx_nested_tensor_from_tensor_list() instead\n",
        "            return _onnx_nested_tensor_from_tensor_list(tensor_list)\n",
        "\n",
        "        # TODO make it support different-sized images\n",
        "        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n",
        "        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n",
        "        batch_shape = [len(tensor_list)] + max_size\n",
        "        b, c, h, w = batch_shape\n",
        "        dtype = tensor_list[0].dtype\n",
        "        device = tensor_list[0].device\n",
        "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
        "        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
        "        for img, pad_img, m in zip(tensor_list, tensor, mask):\n",
        "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
        "            m[: img.shape[1], :img.shape[2]] = False\n",
        "    else:\n",
        "        raise ValueError('not supported')\n",
        "    return NestedTensor(tensor, mask)\n",
        "\n",
        "\n",
        "# _onnx_nested_tensor_from_tensor_list() is an implementation of\n",
        "# nested_tensor_from_tensor_list() that is supported by ONNX tracing.\n",
        "@torch.jit.unused\n",
        "def _onnx_nested_tensor_from_tensor_list(tensor_list: List[Tensor]) -> NestedTensor:\n",
        "    max_size = []\n",
        "    for i in range(tensor_list[0].dim()):\n",
        "        max_size_i = torch.max(torch.stack([img.shape[i] for img in tensor_list]).to(torch.float32)).to(torch.int64)\n",
        "        max_size.append(max_size_i)\n",
        "    max_size = tuple(max_size)\n",
        "\n",
        "    # work around for\n",
        "    # pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
        "    # m[: img.shape[1], :img.shape[2]] = False\n",
        "    # which is not yet supported in onnx\n",
        "    padded_imgs = []\n",
        "    padded_masks = []\n",
        "    for img in tensor_list:\n",
        "        padding = [(s1 - s2) for s1, s2 in zip(max_size, tuple(img.shape))]\n",
        "        padded_img = torch.nn.functional.pad(img, (0, padding[2], 0, padding[1], 0, padding[0]))\n",
        "        padded_imgs.append(padded_img)\n",
        "\n",
        "        m = torch.zeros_like(img[0], dtype=torch.int, device=img.device)\n",
        "        padded_mask = torch.nn.functional.pad(m, (0, padding[2], 0, padding[1]), \"constant\", 1)\n",
        "        padded_masks.append(padded_mask.to(torch.bool))\n",
        "\n",
        "    tensor = torch.stack(padded_imgs)\n",
        "    mask = torch.stack(padded_masks)\n",
        "\n",
        "    return NestedTensor(tensor, mask=mask)\n",
        "\n",
        "\n",
        "def setup_for_distributed(is_master):\n",
        "    \"\"\"\n",
        "    This function disables printing when not in master process\n",
        "    \"\"\"\n",
        "    import builtins as __builtin__\n",
        "    builtin_print = __builtin__.print\n",
        "\n",
        "    def print(*args, **kwargs):\n",
        "        force = kwargs.pop('force', False)\n",
        "        if is_master or force:\n",
        "            builtin_print(*args, **kwargs)\n",
        "\n",
        "    __builtin__.print = print\n",
        "\n",
        "\n",
        "def is_dist_avail_and_initialized():\n",
        "    if not dist.is_available():\n",
        "        return False\n",
        "    if not dist.is_initialized():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "\n",
        "\n",
        "def save_on_master(*args, **kwargs):\n",
        "    if is_main_process():\n",
        "        torch.save(*args, **kwargs)\n",
        "\n",
        "\n",
        "def init_distributed_mode(args):\n",
        "    if 'WORLD_SIZE' in os.environ and os.environ['WORLD_SIZE'] != '': # 'RANK' in os.environ and\n",
        "        # args.rank = int(os.environ[\"RANK\"])\n",
        "        # args.world_size = int(os.environ['WORLD_SIZE'])\n",
        "        # args.gpu = args.local_rank = int(os.environ['LOCAL_RANK'])\n",
        "\n",
        "        # launch by torch.distributed.launch\n",
        "        # Single node\n",
        "        #   python -m torch.distributed.launch --nproc_per_node=8 main.py --world-size 1 --rank 0 ...\n",
        "        # Multi nodes\n",
        "        #   python -m torch.distributed.launch --nproc_per_node=8 main.py --world-size 2 --rank 0 --dist-url 'tcp://IP_OF_NODE0:FREEPORT' ...\n",
        "        #   python -m torch.distributed.launch --nproc_per_node=8 main.py --world-size 2 --rank 1 --dist-url 'tcp://IP_OF_NODE0:FREEPORT' ...\n",
        "\n",
        "        local_world_size = int(os.environ['WORLD_SIZE'])\n",
        "        args.world_size = args.world_size * local_world_size\n",
        "        args.gpu = args.local_rank = int(os.environ['LOCAL_RANK'])\n",
        "        args.rank = args.rank * local_world_size + args.local_rank\n",
        "        print('world size: {}, rank: {}, local rank: {}'.format(args.world_size, args.rank, args.local_rank))\n",
        "        print(json.dumps(dict(os.environ), indent=2))\n",
        "    elif 'SLURM_PROCID' in os.environ:\n",
        "        args.rank = int(os.environ['SLURM_PROCID'])\n",
        "        args.gpu = args.local_rank = int(os.environ['SLURM_LOCALID'])\n",
        "        args.world_size = int(os.environ['SLURM_NPROCS'])\n",
        "\n",
        "        print('world size: {}, world rank: {}, local rank: {}, device_count: {}'.format(args.world_size, args.rank, args.local_rank, torch.cuda.device_count()))\n",
        "    else:\n",
        "        print('Not using distributed mode')\n",
        "        args.distributed = False\n",
        "        args.world_size = 1\n",
        "        args.rank = 0\n",
        "        args.local_rank = 0\n",
        "        return\n",
        "\n",
        "    print(\"world_size:{} rank:{} local_rank:{}\".format(args.world_size, args.rank, args.local_rank))\n",
        "    args.distributed = True\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    args.dist_backend = 'nccl'\n",
        "    print('| distributed init (rank {}): {}'.format(args.rank, args.dist_url), flush=True)\n",
        "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                         world_size=args.world_size, rank=args.rank)\n",
        "    print(\"Before torch.distributed.barrier()\")\n",
        "    torch.distributed.barrier()\n",
        "    print(\"End torch.distributed.barrier()\")\n",
        "    setup_for_distributed(args.rank == 0)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    if target.numel() == 0:\n",
        "        return [torch.zeros([], device=output.device)]\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "def interpolate(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n",
        "    # type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool]) -> Tensor\n",
        "    \"\"\"\n",
        "    Equivalent to nn.functional.interpolate, but with support for empty batch sizes.\n",
        "    This will eventually be supported natively by PyTorch, and this\n",
        "    class can go away.\n",
        "    \"\"\"\n",
        "    if __torchvision_need_compat_flag < 0.7:\n",
        "        if input.numel() > 0:\n",
        "            return torch.nn.functional.interpolate(\n",
        "                input, size, scale_factor, mode, align_corners\n",
        "            )\n",
        "\n",
        "        output_shape = _output_size(2, input, size, scale_factor)\n",
        "        output_shape = list(input.shape[:-2]) + list(output_shape)\n",
        "        return _new_empty_tensor(input, output_shape)\n",
        "    else:\n",
        "        return torchvision.ops.misc.interpolate(input, size, scale_factor, mode, align_corners)\n",
        "\n",
        "\n",
        "\n",
        "class color_sys():\n",
        "    def __init__(self, num_colors) -> None:\n",
        "        self.num_colors = num_colors\n",
        "        colors=[]\n",
        "        for i in np.arange(0., 360., 360. / num_colors):\n",
        "            hue = i/360.\n",
        "            lightness = (50 + np.random.rand() * 10)/100.\n",
        "            saturation = (90 + np.random.rand() * 10)/100.\n",
        "            colors.append(tuple([int(j*255) for j in colorsys.hls_to_rgb(hue, lightness, saturation)]))\n",
        "        self.colors = colors\n",
        "\n",
        "    def __call__(self, idx):\n",
        "        return self.colors[idx]\n",
        "\n",
        "def inverse_sigmoid(x, eps=1e-3):\n",
        "    x = x.clamp(min=0, max=1)\n",
        "    x1 = x.clamp(min=eps)\n",
        "    x2 = (1 - x).clamp(min=eps)\n",
        "    return torch.log(x1/x2)\n",
        "\n",
        "def clean_state_dict(state_dict):\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        if k[:7] == 'module.':\n",
        "            k = k[7:]  # remove `module.`\n",
        "        new_state_dict[k] = v\n",
        "    return new_state_dict"
      ],
      "metadata": {
        "id": "Fl-ajRnkKtyl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Modified from mmcv\n",
        "# ==========================================================\n",
        "import os, sys\n",
        "import os.path as osp\n",
        "import ast\n",
        "import tempfile\n",
        "import shutil\n",
        "from importlib import import_module\n",
        "\n",
        "from argparse import Action\n",
        "\n",
        "from addict import Dict\n",
        "from yapf.yapflib.yapf_api import FormatCode\n",
        "\n",
        "import platform\n",
        "MACOS, LINUX, WINDOWS = (platform.system() == x for x in ['Darwin', 'Linux', 'Windows'])  # environment booleans\n",
        "\n",
        "BASE_KEY = '_base_'\n",
        "DELETE_KEY = '_delete_'\n",
        "RESERVED_KEYS = ['filename', 'text', 'pretty_text', 'get', 'dump', 'merge_from_dict']\n",
        "\n",
        "filename = '/content/DINO/config/DINO/DINO_4scale.py'\n",
        "def check_file_exist(filename, msg_tmpl='file \"{}\" does not exist'):\n",
        "    if not osp.isfile(filename):\n",
        "        raise FileNotFoundError(msg_tmpl.format(filename))\n",
        "\n",
        "class ConfigDict(Dict):\n",
        "\n",
        "    def __missing__(self, name):\n",
        "        raise KeyError(name)\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        try:\n",
        "            value = super(ConfigDict, self).__getattr__(name)\n",
        "        except KeyError:\n",
        "            ex = AttributeError(f\"'{self.__class__.__name__}' object has no \"\n",
        "                                f\"attribute '{name}'\")\n",
        "        except Exception as e:\n",
        "            ex = e\n",
        "        else:\n",
        "            return value\n",
        "        raise ex\n",
        "\n",
        "\n",
        "class SLConfig(object):\n",
        "    \"\"\"\n",
        "    config files.\n",
        "    only support .py file as config now.\n",
        "\n",
        "    ref: mmcv.utils.config\n",
        "\n",
        "    Example:\n",
        "        >>> cfg = Config(dict(a=1, b=dict(b1=[0, 1])))\n",
        "        >>> cfg.a\n",
        "        1\n",
        "        >>> cfg.b\n",
        "        {'b1': [0, 1]}\n",
        "        >>> cfg.b.b1\n",
        "        [0, 1]\n",
        "        >>> cfg = Config.fromfile('tests/data/config/a.py')\n",
        "        >>> cfg.filename\n",
        "        \"/home/kchen/projects/mmcv/tests/data/config/a.py\"\n",
        "        >>> cfg.item4\n",
        "        'test'\n",
        "        >>> cfg\n",
        "        \"Config [path: /home/kchen/projects/mmcv/tests/data/config/a.py]: \"\n",
        "        \"{'item1': [1, 2], 'item2': {'a': 0}, 'item3': True, 'item4': 'test'}\"\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def _validate_py_syntax(filename):\n",
        "        with open(filename) as f:\n",
        "            content = f.read()\n",
        "        try:\n",
        "            ast.parse(content)\n",
        "        except SyntaxError:\n",
        "            raise SyntaxError('There are syntax errors in config '\n",
        "                              f'file {filename}')\n",
        "\n",
        "    @staticmethod\n",
        "    def _file2dict(filename):\n",
        "        filename = osp.abspath(osp.expanduser(filename))\n",
        "        check_file_exist(filename)\n",
        "        if filename.lower().endswith('.py'):\n",
        "            with tempfile.TemporaryDirectory() as temp_config_dir:\n",
        "                temp_config_file = tempfile.NamedTemporaryFile(\n",
        "                    dir=temp_config_dir, suffix='.py')\n",
        "                temp_config_name = osp.basename(temp_config_file.name)\n",
        "                if WINDOWS:\n",
        "                    temp_config_file.close()\n",
        "                shutil.copyfile(filename,\n",
        "                                osp.join(temp_config_dir, temp_config_name))\n",
        "                temp_module_name = osp.splitext(temp_config_name)[0]\n",
        "                sys.path.insert(0, temp_config_dir)\n",
        "                SLConfig._validate_py_syntax(filename)\n",
        "                mod = import_module(temp_module_name)\n",
        "                sys.path.pop(0)\n",
        "                cfg_dict = {\n",
        "                    name: value\n",
        "                    for name, value in mod.__dict__.items()\n",
        "                    if not name.startswith('__')\n",
        "                }\n",
        "                # delete imported module\n",
        "                del sys.modules[temp_module_name]\n",
        "                # close temp file\n",
        "                temp_config_file.close()\n",
        "        elif filename.lower().endswith(('.yml', '.yaml', '.json')):\n",
        "            from .slio import slload\n",
        "            cfg_dict = slload(filename)\n",
        "        else:\n",
        "            raise IOError('Only py/yml/yaml/json type are supported now!')\n",
        "\n",
        "        cfg_text = filename + '\\n'\n",
        "        with open(filename, 'r') as f:\n",
        "            cfg_text += f.read()\n",
        "\n",
        "        # parse the base file\n",
        "        if BASE_KEY in cfg_dict:\n",
        "            cfg_dir = osp.dirname(filename)\n",
        "            base_filename = cfg_dict.pop(BASE_KEY)\n",
        "            base_filename = base_filename if isinstance(\n",
        "                base_filename, list) else [base_filename]\n",
        "\n",
        "            cfg_dict_list = list()\n",
        "            cfg_text_list = list()\n",
        "            for f in base_filename:\n",
        "                _cfg_dict, _cfg_text = SLConfig._file2dict(osp.join(cfg_dir, f))\n",
        "                cfg_dict_list.append(_cfg_dict)\n",
        "                cfg_text_list.append(_cfg_text)\n",
        "\n",
        "            base_cfg_dict = dict()\n",
        "            for c in cfg_dict_list:\n",
        "                if len(base_cfg_dict.keys() & c.keys()) > 0:\n",
        "                    raise KeyError('Duplicate key is not allowed among bases')\n",
        "                    # TODO Allow the duplicate key while warnning user\n",
        "                base_cfg_dict.update(c)\n",
        "\n",
        "            base_cfg_dict = SLConfig._merge_a_into_b(cfg_dict, base_cfg_dict)\n",
        "            cfg_dict = base_cfg_dict\n",
        "\n",
        "            # merge cfg_text\n",
        "            cfg_text_list.append(cfg_text)\n",
        "            cfg_text = '\\n'.join(cfg_text_list)\n",
        "\n",
        "        return cfg_dict, cfg_text\n",
        "\n",
        "    @staticmethod\n",
        "    def _merge_a_into_b(a, b):\n",
        "        \"\"\"merge dict `a` into dict `b` (non-inplace).\n",
        "            values in `a` will overwrite `b`.\n",
        "            copy first to avoid inplace modification\n",
        "\n",
        "        Args:\n",
        "            a ([type]): [description]\n",
        "            b ([type]): [description]\n",
        "\n",
        "        Returns:\n",
        "            [dict]: [description]\n",
        "        \"\"\"\n",
        "\n",
        "        if not isinstance(a, dict):\n",
        "            return a\n",
        "\n",
        "        b = b.copy()\n",
        "        for k, v in a.items():\n",
        "            if isinstance(v, dict) and k in b and not v.pop(DELETE_KEY, False):\n",
        "\n",
        "                if not isinstance(b[k], dict) and not isinstance(b[k], list):\n",
        "                    # if :\n",
        "\n",
        "                    raise TypeError(\n",
        "                        f'{k}={v} in child config cannot inherit from base '\n",
        "                        f'because {k} is a dict in the child config but is of '\n",
        "                        f'type {type(b[k])} in base config. You may set '\n",
        "                        f'`{DELETE_KEY}=True` to ignore the base config')\n",
        "                b[k] = SLConfig._merge_a_into_b(v, b[k])\n",
        "            elif isinstance(b, list):\n",
        "                try:\n",
        "                    _ = int(k)\n",
        "                except:\n",
        "                    raise TypeError(\n",
        "                        f'b is a list, '\n",
        "                        f'index {k} should be an int when input but {type(k)}'\n",
        "                    )\n",
        "                b[int(k)] = SLConfig._merge_a_into_b(v, b[int(k)])\n",
        "            else:\n",
        "                b[k] = v\n",
        "\n",
        "        return b\n",
        "\n",
        "    @staticmethod\n",
        "    def fromfile(filename):\n",
        "        cfg_dict, cfg_text = SLConfig._file2dict(filename)\n",
        "        return SLConfig(cfg_dict, cfg_text=cfg_text, filename=filename)\n",
        "\n",
        "\n",
        "    def __init__(self, cfg_dict=None, cfg_text=None, filename=None):\n",
        "        if cfg_dict is None:\n",
        "            cfg_dict = dict()\n",
        "        elif not isinstance(cfg_dict, dict):\n",
        "            raise TypeError('cfg_dict must be a dict, but '\n",
        "                            f'got {type(cfg_dict)}')\n",
        "        for key in cfg_dict:\n",
        "            if key in RESERVED_KEYS:\n",
        "                raise KeyError(f'{key} is reserved for config file')\n",
        "\n",
        "        super(SLConfig, self).__setattr__('_cfg_dict', ConfigDict(cfg_dict))\n",
        "        super(SLConfig, self).__setattr__('_filename', filename)\n",
        "        if cfg_text:\n",
        "            text = cfg_text\n",
        "        elif filename:\n",
        "            with open(filename, 'r') as f:\n",
        "                text = f.read()\n",
        "        else:\n",
        "            text = ''\n",
        "        super(SLConfig, self).__setattr__('_text', text)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def filename(self):\n",
        "        return self._filename\n",
        "\n",
        "    @property\n",
        "    def text(self):\n",
        "        return self._text\n",
        "\n",
        "    @property\n",
        "    def pretty_text(self):\n",
        "\n",
        "        indent = 4\n",
        "\n",
        "        def _indent(s_, num_spaces):\n",
        "            s = s_.split('\\n')\n",
        "            if len(s) == 1:\n",
        "                return s_\n",
        "            first = s.pop(0)\n",
        "            s = [(num_spaces * ' ') + line for line in s]\n",
        "            s = '\\n'.join(s)\n",
        "            s = first + '\\n' + s\n",
        "            return s\n",
        "\n",
        "        def _format_basic_types(k, v, use_mapping=False):\n",
        "            if isinstance(v, str):\n",
        "                v_str = f\"'{v}'\"\n",
        "            else:\n",
        "                v_str = str(v)\n",
        "\n",
        "            if use_mapping:\n",
        "                k_str = f\"'{k}'\" if isinstance(k, str) else str(k)\n",
        "                attr_str = f'{k_str}: {v_str}'\n",
        "            else:\n",
        "                attr_str = f'{str(k)}={v_str}'\n",
        "            attr_str = _indent(attr_str, indent)\n",
        "\n",
        "            return attr_str\n",
        "\n",
        "        def _format_list(k, v, use_mapping=False):\n",
        "            # check if all items in the list are dict\n",
        "            if all(isinstance(_, dict) for _ in v):\n",
        "                v_str = '[\\n'\n",
        "                v_str += '\\n'.join(\n",
        "                    f'dict({_indent(_format_dict(v_), indent)}),'\n",
        "                    for v_ in v).rstrip(',')\n",
        "                if use_mapping:\n",
        "                    k_str = f\"'{k}'\" if isinstance(k, str) else str(k)\n",
        "                    attr_str = f'{k_str}: {v_str}'\n",
        "                else:\n",
        "                    attr_str = f'{str(k)}={v_str}'\n",
        "                attr_str = _indent(attr_str, indent) + ']'\n",
        "            else:\n",
        "                attr_str = _format_basic_types(k, v, use_mapping)\n",
        "            return attr_str\n",
        "\n",
        "        def _contain_invalid_identifier(dict_str):\n",
        "            contain_invalid_identifier = False\n",
        "            for key_name in dict_str:\n",
        "                contain_invalid_identifier |= \\\n",
        "                    (not str(key_name).isidentifier())\n",
        "            return contain_invalid_identifier\n",
        "\n",
        "        def _format_dict(input_dict, outest_level=False):\n",
        "            r = ''\n",
        "            s = []\n",
        "\n",
        "            use_mapping = _contain_invalid_identifier(input_dict)\n",
        "            if use_mapping:\n",
        "                r += '{'\n",
        "            for idx, (k, v) in enumerate(input_dict.items()):\n",
        "                is_last = idx >= len(input_dict) - 1\n",
        "                end = '' if outest_level or is_last else ','\n",
        "                if isinstance(v, dict):\n",
        "                    v_str = '\\n' + _format_dict(v)\n",
        "                    if use_mapping:\n",
        "                        k_str = f\"'{k}'\" if isinstance(k, str) else str(k)\n",
        "                        attr_str = f'{k_str}: dict({v_str}'\n",
        "                    else:\n",
        "                        attr_str = f'{str(k)}=dict({v_str}'\n",
        "                    attr_str = _indent(attr_str, indent) + ')' + end\n",
        "                elif isinstance(v, list):\n",
        "                    attr_str = _format_list(k, v, use_mapping) + end\n",
        "                else:\n",
        "                    attr_str = _format_basic_types(k, v, use_mapping) + end\n",
        "\n",
        "                s.append(attr_str)\n",
        "            r += '\\n'.join(s)\n",
        "            if use_mapping:\n",
        "                r += '}'\n",
        "            return r\n",
        "\n",
        "        cfg_dict = self._cfg_dict.to_dict()\n",
        "        text = _format_dict(cfg_dict, outest_level=True)\n",
        "        # copied from setup.cfg\n",
        "        yapf_style = dict(\n",
        "            based_on_style='pep8',\n",
        "            blank_line_before_nested_class_or_def=True,\n",
        "            split_before_expression_after_opening_paren=True)\n",
        "        text, _ = FormatCode(text, style_config=yapf_style, verify=True)\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'Config (path: {self.filename}): {self._cfg_dict.__repr__()}'\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._cfg_dict)\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        # # debug\n",
        "        # print('+'*15)\n",
        "        # print('name=%s' % name)\n",
        "        # print(\"addr:\", id(self))\n",
        "        # # print('type(self):', type(self))\n",
        "        # print(self.__dict__)\n",
        "        # print('+'*15)\n",
        "        # if self.__dict__ == {}:\n",
        "        #     raise ValueError\n",
        "\n",
        "        return getattr(self._cfg_dict, name)\n",
        "\n",
        "    def __getitem__(self, name):\n",
        "        return self._cfg_dict.__getitem__(name)\n",
        "\n",
        "    def __setattr__(self, name, value):\n",
        "        if isinstance(value, dict):\n",
        "            value = ConfigDict(value)\n",
        "        self._cfg_dict.__setattr__(name, value)\n",
        "\n",
        "    def __setitem__(self, name, value):\n",
        "        if isinstance(value, dict):\n",
        "            value = ConfigDict(value)\n",
        "        self._cfg_dict.__setitem__(name, value)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self._cfg_dict)\n",
        "\n",
        "    def dump(self, file=None):\n",
        "\n",
        "        if file is None:\n",
        "            return self.pretty_text\n",
        "        else:\n",
        "            with open(file, 'w') as f:\n",
        "                f.write(self.pretty_text)\n",
        "\n",
        "    def merge_from_dict(self, options):\n",
        "        \"\"\"Merge list into cfg_dict\n",
        "\n",
        "        Merge the dict parsed by MultipleKVAction into this cfg.\n",
        "\n",
        "        Examples:\n",
        "            >>> options = {'model.backbone.depth': 50,\n",
        "            ...            'model.backbone.with_cp':True}\n",
        "            >>> cfg = Config(dict(model=dict(backbone=dict(type='ResNet'))))\n",
        "            >>> cfg.merge_from_dict(options)\n",
        "            >>> cfg_dict = super(Config, self).__getattribute__('_cfg_dict')\n",
        "            >>> assert cfg_dict == dict(\n",
        "            ...     model=dict(backbone=dict(depth=50, with_cp=True)))\n",
        "\n",
        "        Args:\n",
        "            options (dict): dict of configs to merge from.\n",
        "        \"\"\"\n",
        "        option_cfg_dict = {}\n",
        "        for full_key, v in options.items():\n",
        "            d = option_cfg_dict\n",
        "            key_list = full_key.split('.')\n",
        "            for subkey in key_list[:-1]:\n",
        "                d.setdefault(subkey, ConfigDict())\n",
        "                d = d[subkey]\n",
        "            subkey = key_list[-1]\n",
        "            d[subkey] = v\n",
        "\n",
        "        cfg_dict = super(SLConfig, self).__getattribute__('_cfg_dict')\n",
        "        super(SLConfig, self).__setattr__(\n",
        "            '_cfg_dict', SLConfig._merge_a_into_b(option_cfg_dict, cfg_dict))\n",
        "\n",
        "    # for multiprocess\n",
        "    def __setstate__(self, state):\n",
        "        self.__init__(state)\n",
        "\n",
        "\n",
        "    def copy(self):\n",
        "        return SLConfig(self._cfg_dict.copy())\n",
        "\n",
        "    def deepcopy(self):\n",
        "        return SLConfig(self._cfg_dict.deepcopy())\n",
        "\n",
        "\n",
        "class DictAction(Action):\n",
        "    \"\"\"\n",
        "    argparse action to split an argument into KEY=VALUE form\n",
        "    on the first = and append to a dictionary. List options should\n",
        "    be passed as comma separated values, i.e KEY=V1,V2,V3\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _parse_int_float_bool(val):\n",
        "        try:\n",
        "            return int(val)\n",
        "        except ValueError:\n",
        "            pass\n",
        "        try:\n",
        "            return float(val)\n",
        "        except ValueError:\n",
        "            pass\n",
        "        if val.lower() in ['true', 'false']:\n",
        "            return True if val.lower() == 'true' else False\n",
        "        if val.lower() in ['none', 'null']:\n",
        "            return None\n",
        "        return val\n",
        "\n",
        "    def __call__(self, parser, namespace, values, option_string=None):\n",
        "        options = {}\n",
        "        for kv in values:\n",
        "            key, val = kv.split('=', maxsplit=1)\n",
        "            val = [self._parse_int_float_bool(v) for v in val.split(',')]\n",
        "            if len(val) == 1:\n",
        "                val = val[0]\n",
        "            options[key] = val\n",
        "        setattr(namespace, self.dest, options)\n",
        "\n"
      ],
      "metadata": {
        "id": "O_Vytpp-LN9D"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "from copy import deepcopy\n",
        "import json\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def slprint(x, name='x'):\n",
        "    if isinstance(x, (torch.Tensor, np.ndarray)):\n",
        "        print(f'{name}.shape:', x.shape)\n",
        "    elif isinstance(x, (tuple, list)):\n",
        "        print('type x:', type(x))\n",
        "        for i in range(min(10, len(x))):\n",
        "            slprint(x[i], f'{name}[{i}]')\n",
        "    elif isinstance(x, dict):\n",
        "        for k,v in x.items():\n",
        "            slprint(v, f'{name}[{k}]')\n",
        "    else:\n",
        "        print(f'{name}.type:', type(x))\n",
        "\n",
        "def clean_state_dict(state_dict):\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        if k[:7] == 'module.':\n",
        "            k = k[7:]  # remove `module.`\n",
        "        new_state_dict[k] = v\n",
        "    return new_state_dict\n",
        "\n",
        "def renorm(img: torch.FloatTensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \\\n",
        "        -> torch.FloatTensor:\n",
        "    # img: tensor(3,H,W) or tensor(B,3,H,W)\n",
        "    # return: same as img\n",
        "    assert img.dim() == 3 or img.dim() == 4, \"img.dim() should be 3 or 4 but %d\" % img.dim()\n",
        "    if img.dim() == 3:\n",
        "        assert img.size(0) == 3, 'img.size(0) shoule be 3 but \"%d\". (%s)' % (img.size(0), str(img.size()))\n",
        "        img_perm = img.permute(1,2,0)\n",
        "        mean = torch.Tensor(mean)\n",
        "        std = torch.Tensor(std)\n",
        "        img_res = img_perm * std + mean\n",
        "        return img_res.permute(2,0,1)\n",
        "    else: # img.dim() == 4\n",
        "        assert img.size(1) == 3, 'img.size(1) shoule be 3 but \"%d\". (%s)' % (img.size(1), str(img.size()))\n",
        "        img_perm = img.permute(0,2,3,1)\n",
        "        mean = torch.Tensor(mean)\n",
        "        std = torch.Tensor(std)\n",
        "        img_res = img_perm * std + mean\n",
        "        return img_res.permute(0,3,1,2)\n",
        "\n",
        "\n",
        "\n",
        "class CocoClassMapper():\n",
        "    def __init__(self) -> None:\n",
        "        self.category_map_str = {\"1\": 1, \"2\": 2, \"3\": 3, \"4\": 4, \"5\": 5, \"6\": 6, \"7\": 7, \"8\": 8, \"9\": 9, \"10\": 10, \"11\": 11, \"13\": 12, \"14\": 13, \"15\": 14, \"16\": 15, \"17\": 16, \"18\": 17, \"19\": 18, \"20\": 19, \"21\": 20, \"22\": 21, \"23\": 22, \"24\": 23, \"25\": 24, \"27\": 25, \"28\": 26, \"31\": 27, \"32\": 28, \"33\": 29, \"34\": 30, \"35\": 31, \"36\": 32, \"37\": 33, \"38\": 34, \"39\": 35, \"40\": 36, \"41\": 37, \"42\": 38, \"43\": 39, \"44\": 40, \"46\": 41, \"47\": 42, \"48\": 43, \"49\": 44, \"50\": 45, \"51\": 46, \"52\": 47, \"53\": 48, \"54\": 49, \"55\": 50, \"56\": 51, \"57\": 52, \"58\": 53, \"59\": 54, \"60\": 55, \"61\": 56, \"62\": 57, \"63\": 58, \"64\": 59, \"65\": 60, \"67\": 61, \"70\": 62, \"72\": 63, \"73\": 64, \"74\": 65, \"75\": 66, \"76\": 67, \"77\": 68, \"78\": 69, \"79\": 70, \"80\": 71, \"81\": 72, \"82\": 73, \"84\": 74, \"85\": 75, \"86\": 76, \"87\": 77, \"88\": 78, \"89\": 79, \"90\": 80}\n",
        "        self.origin2compact_mapper = {int(k):v-1 for k,v in self.category_map_str.items()}\n",
        "        self.compact2origin_mapper = {int(v-1):int(k) for k,v in self.category_map_str.items()}\n",
        "\n",
        "    def origin2compact(self, idx):\n",
        "        return self.origin2compact_mapper[int(idx)]\n",
        "\n",
        "    def compact2origin(self, idx):\n",
        "        return self.compact2origin_mapper[int(idx)]\n",
        "\n",
        "def to_device(item, device):\n",
        "    if isinstance(item, torch.Tensor):\n",
        "        return item.to(device)\n",
        "    elif isinstance(item, list):\n",
        "        return [to_device(i, device) for i in item]\n",
        "    elif isinstance(item, dict):\n",
        "        return {k: to_device(v, device) for k,v in item.items()}\n",
        "    else:\n",
        "        raise NotImplementedError(\"Call Shilong if you use other containers! type: {}\".format(type(item)))\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "def get_gaussian_mean(x, axis, other_axis, softmax=True):\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "        x (float): Input images(BxCxHxW)\n",
        "        axis (int): The index for weighted mean\n",
        "        other_axis (int): The other index\n",
        "\n",
        "    Returns: weighted index for axis, BxC\n",
        "\n",
        "    \"\"\"\n",
        "    mat2line = torch.sum(x, axis=other_axis)\n",
        "    # mat2line = mat2line / mat2line.mean() * 10\n",
        "    if softmax:\n",
        "        u = torch.softmax(mat2line, axis=2)\n",
        "    else:\n",
        "        u = mat2line / (mat2line.sum(2, keepdim=True) + 1e-6)\n",
        "    size = x.shape[axis]\n",
        "    ind = torch.linspace(0, 1, size).to(x.device)\n",
        "    batch = x.shape[0]\n",
        "    channel = x.shape[1]\n",
        "    index = ind.repeat([batch, channel, 1])\n",
        "    mean_position = torch.sum(index * u, dim=2)\n",
        "    return mean_position\n",
        "\n",
        "def get_expected_points_from_map(hm, softmax=True):\n",
        "    \"\"\"get_gaussian_map_from_points\n",
        "        B,C,H,W -> B,N,2 float(0, 1) float(0, 1)\n",
        "        softargmax function\n",
        "\n",
        "    Args:\n",
        "        hm (float): Input images(BxCxHxW)\n",
        "\n",
        "    Returns:\n",
        "        weighted index for axis, BxCx2. float between 0 and 1.\n",
        "\n",
        "    \"\"\"\n",
        "    # hm = 10*hm\n",
        "    B,C,H,W = hm.shape\n",
        "    y_mean = get_gaussian_mean(hm, 2, 3, softmax=softmax) # B,C\n",
        "    x_mean = get_gaussian_mean(hm, 3, 2, softmax=softmax) # B,C\n",
        "    # return torch.cat((x_mean.unsqueeze(-1), y_mean.unsqueeze(-1)), 2)\n",
        "    return torch.stack([x_mean, y_mean], dim=2)\n",
        "\n",
        "# Positional encoding (section 5.1)\n",
        "# borrow from nerf\n",
        "class Embedder:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.kwargs = kwargs\n",
        "        self.create_embedding_fn()\n",
        "\n",
        "    def create_embedding_fn(self):\n",
        "        embed_fns = []\n",
        "        d = self.kwargs['input_dims']\n",
        "        out_dim = 0\n",
        "        if self.kwargs['include_input']:\n",
        "            embed_fns.append(lambda x : x)\n",
        "            out_dim += d\n",
        "\n",
        "        max_freq = self.kwargs['max_freq_log2']\n",
        "        N_freqs = self.kwargs['num_freqs']\n",
        "\n",
        "        if self.kwargs['log_sampling']:\n",
        "            freq_bands = 2.**torch.linspace(0., max_freq, steps=N_freqs)\n",
        "        else:\n",
        "            freq_bands = torch.linspace(2.**0., 2.**max_freq, steps=N_freqs)\n",
        "\n",
        "        for freq in freq_bands:\n",
        "            for p_fn in self.kwargs['periodic_fns']:\n",
        "                embed_fns.append(lambda x, p_fn=p_fn, freq=freq : p_fn(x * freq))\n",
        "                out_dim += d\n",
        "\n",
        "        self.embed_fns = embed_fns\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "    def embed(self, inputs):\n",
        "        return torch.cat([fn(inputs) for fn in self.embed_fns], -1)\n",
        "\n",
        "\n",
        "def get_embedder(multires, i=0):\n",
        "    import torch.nn as nn\n",
        "    if i == -1:\n",
        "        return nn.Identity(), 3\n",
        "\n",
        "    embed_kwargs = {\n",
        "                'include_input' : True,\n",
        "                'input_dims' : 3,\n",
        "                'max_freq_log2' : multires-1,\n",
        "                'num_freqs' : multires,\n",
        "                'log_sampling' : True,\n",
        "                'periodic_fns' : [torch.sin, torch.cos],\n",
        "    }\n",
        "\n",
        "    embedder_obj = Embedder(**embed_kwargs)\n",
        "    embed = lambda x, eo=embedder_obj : eo.embed(x)\n",
        "    return embed, embedder_obj.out_dim\n",
        "\n",
        "class APOPMeter():\n",
        "    def __init__(self) -> None:\n",
        "        self.tp = 0\n",
        "        self.fp = 0\n",
        "        self.tn = 0\n",
        "        self.fn = 0\n",
        "\n",
        "    def update(self, pred, gt):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            pred, gt: Tensor()\n",
        "        \"\"\"\n",
        "        assert pred.shape == gt.shape\n",
        "        self.tp += torch.logical_and(pred == 1, gt == 1).sum().item()\n",
        "        self.fp += torch.logical_and(pred == 1, gt == 0).sum().item()\n",
        "        self.tn += torch.logical_and(pred == 0, gt == 0).sum().item()\n",
        "        self.tn += torch.logical_and(pred == 1, gt == 0).sum().item()\n",
        "\n",
        "    def update_cm(self, tp, fp, tn, fn):\n",
        "        self.tp += tp\n",
        "        self.fp += fp\n",
        "        self.tn += tn\n",
        "        self.tn += fn\n",
        "\n",
        "def inverse_sigmoid(x, eps=1e-5):\n",
        "    x = x.clamp(min=0, max=1)\n",
        "    x1 = x.clamp(min=eps)\n",
        "    x2 = (1 - x).clamp(min=eps)\n",
        "    return torch.log(x1/x2)\n",
        "\n",
        "import argparse\n",
        "\n",
        "def get_raw_dict(args):\n",
        "    \"\"\"\n",
        "    return the dicf contained in args.\n",
        "\n",
        "    e.g:\n",
        "        >>> with open(path, 'w') as f:\n",
        "                json.dump(get_raw_dict(args), f, indent=2)\n",
        "    \"\"\"\n",
        "    if isinstance(args, argparse.Namespace):\n",
        "        return vars(args)\n",
        "    elif isinstance(args, dict):\n",
        "        return args\n",
        "    elif isinstance(args, SLConfig):\n",
        "        return args._cfg_dict\n",
        "    else:\n",
        "        raise NotImplementedError(\"Unknown type {}\".format(type(args)))\n",
        "\n",
        "\n",
        "def stat_tensors(tensor):\n",
        "    assert tensor.dim() == 1\n",
        "    tensor_sm = tensor.softmax(0)\n",
        "    entropy = (tensor_sm * torch.log(tensor_sm + 1e-9)).sum()\n",
        "\n",
        "    return {\n",
        "        'max': tensor.max(),\n",
        "        'min': tensor.min(),\n",
        "        'mean': tensor.mean(),\n",
        "        'var': tensor.var(),\n",
        "        'std': tensor.var() ** 0.5,\n",
        "        'entropy': entropy\n",
        "    }\n",
        "\n",
        "\n",
        "class NiceRepr:\n",
        "    \"\"\"Inherit from this class and define ``__nice__`` to \"nicely\" print your\n",
        "    objects.\n",
        "\n",
        "    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function\n",
        "    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.\n",
        "    If the inheriting class has a ``__len__``, method then the default\n",
        "    ``__nice__`` method will return its length.\n",
        "\n",
        "    Example:\n",
        "        >>> class Foo(NiceRepr):\n",
        "        ...    def __nice__(self):\n",
        "        ...        return 'info'\n",
        "        >>> foo = Foo()\n",
        "        >>> assert str(foo) == '<Foo(info)>'\n",
        "        >>> assert repr(foo).startswith('<Foo(info) at ')\n",
        "\n",
        "    Example:\n",
        "        >>> class Bar(NiceRepr):\n",
        "        ...    pass\n",
        "        >>> bar = Bar()\n",
        "        >>> import pytest\n",
        "        >>> with pytest.warns(None) as record:\n",
        "        >>>     assert 'object at' in str(bar)\n",
        "        >>>     assert 'object at' in repr(bar)\n",
        "\n",
        "    Example:\n",
        "        >>> class Baz(NiceRepr):\n",
        "        ...    def __len__(self):\n",
        "        ...        return 5\n",
        "        >>> baz = Baz()\n",
        "        >>> assert str(baz) == '<Baz(5)>'\n",
        "    \"\"\"\n",
        "\n",
        "    def __nice__(self):\n",
        "        \"\"\"str: a \"nice\" summary string describing this module\"\"\"\n",
        "        if hasattr(self, '__len__'):\n",
        "            # It is a common pattern for objects to use __len__ in __nice__\n",
        "            # As a convenience we define a default __nice__ for these objects\n",
        "            return str(len(self))\n",
        "        else:\n",
        "            # In all other cases force the subclass to overload __nice__\n",
        "            raise NotImplementedError(\n",
        "                f'Define the __nice__ method for {self.__class__!r}')\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"str: the string of the module\"\"\"\n",
        "        try:\n",
        "            nice = self.__nice__()\n",
        "            classname = self.__class__.__name__\n",
        "            return f'<{classname}({nice}) at {hex(id(self))}>'\n",
        "        except NotImplementedError as ex:\n",
        "            warnings.warn(str(ex), category=RuntimeWarning)\n",
        "            return object.__repr__(self)\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"str: the string of the module\"\"\"\n",
        "        try:\n",
        "            classname = self.__class__.__name__\n",
        "            nice = self.__nice__()\n",
        "            return f'<{classname}({nice})>'\n",
        "        except NotImplementedError as ex:\n",
        "            warnings.warn(str(ex), category=RuntimeWarning)\n",
        "            return object.__repr__(self)\n",
        "\n",
        "\n",
        "\n",
        "def ensure_rng(rng=None):\n",
        "    \"\"\"Coerces input into a random number generator.\n",
        "\n",
        "    If the input is None, then a global random state is returned.\n",
        "\n",
        "    If the input is a numeric value, then that is used as a seed to construct a\n",
        "    random state. Otherwise the input is returned as-is.\n",
        "\n",
        "    Adapted from [1]_.\n",
        "\n",
        "    Args:\n",
        "        rng (int | numpy.random.RandomState | None):\n",
        "            if None, then defaults to the global rng. Otherwise this can be an\n",
        "            integer or a RandomState class\n",
        "    Returns:\n",
        "        (numpy.random.RandomState) : rng -\n",
        "            a numpy random number generator\n",
        "\n",
        "    References:\n",
        "        .. [1] https://gitlab.kitware.com/computer-vision/kwarray/blob/master/kwarray/util_random.py#L270  # noqa: E501\n",
        "    \"\"\"\n",
        "\n",
        "    if rng is None:\n",
        "        rng = np.random.mtrand._rand\n",
        "    elif isinstance(rng, int):\n",
        "        rng = np.random.RandomState(rng)\n",
        "    else:\n",
        "        rng = rng\n",
        "    return rng\n",
        "\n",
        "def random_boxes(num=1, scale=1, rng=None):\n",
        "    \"\"\"Simple version of ``kwimage.Boxes.random``\n",
        "\n",
        "    Returns:\n",
        "        Tensor: shape (n, 4) in x1, y1, x2, y2 format.\n",
        "\n",
        "    References:\n",
        "        https://gitlab.kitware.com/computer-vision/kwimage/blob/master/kwimage/structs/boxes.py#L1390\n",
        "\n",
        "    Example:\n",
        "        >>> num = 3\n",
        "        >>> scale = 512\n",
        "        >>> rng = 0\n",
        "        >>> boxes = random_boxes(num, scale, rng)\n",
        "        >>> print(boxes)\n",
        "        tensor([[280.9925, 278.9802, 308.6148, 366.1769],\n",
        "                [216.9113, 330.6978, 224.0446, 456.5878],\n",
        "                [405.3632, 196.3221, 493.3953, 270.7942]])\n",
        "    \"\"\"\n",
        "    rng = ensure_rng(rng)\n",
        "\n",
        "    tlbr = rng.rand(num, 4).astype(np.float32)\n",
        "\n",
        "    tl_x = np.minimum(tlbr[:, 0], tlbr[:, 2])\n",
        "    tl_y = np.minimum(tlbr[:, 1], tlbr[:, 3])\n",
        "    br_x = np.maximum(tlbr[:, 0], tlbr[:, 2])\n",
        "    br_y = np.maximum(tlbr[:, 1], tlbr[:, 3])\n",
        "\n",
        "    tlbr[:, 0] = tl_x * scale\n",
        "    tlbr[:, 1] = tl_y * scale\n",
        "    tlbr[:, 2] = br_x * scale\n",
        "    tlbr[:, 3] = br_y * scale\n",
        "\n",
        "    boxes = torch.from_numpy(tlbr)\n",
        "    return boxes\n",
        "\n",
        "\n",
        "class ModelEma(torch.nn.Module):\n",
        "    def __init__(self, model, decay=0.9997, device=None):\n",
        "        super(ModelEma, self).__init__()\n",
        "        # make a copy of the model for accumulating moving average of weights\n",
        "        self.module = deepcopy(model)\n",
        "        self.module.eval()\n",
        "\n",
        "        self.decay = decay\n",
        "        self.device = device  # perform ema on different device from model if set\n",
        "        if self.device is not None:\n",
        "            self.module.to(device=device)\n",
        "\n",
        "    def _update(self, model, update_fn):\n",
        "        with torch.no_grad():\n",
        "            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n",
        "                if self.device is not None:\n",
        "                    model_v = model_v.to(device=self.device)\n",
        "                ema_v.copy_(update_fn(ema_v, model_v))\n",
        "\n",
        "    def update(self, model):\n",
        "        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n",
        "\n",
        "    def set(self, model):\n",
        "        self._update(model, update_fn=lambda e, m: m)\n",
        "\n",
        "class BestMetricSingle():\n",
        "    def __init__(self, init_res=0.0, better='large') -> None:\n",
        "        self.init_res = init_res\n",
        "        self.best_res = init_res\n",
        "        self.best_ep = -1\n",
        "\n",
        "        self.better = better\n",
        "        assert better in ['large', 'small']\n",
        "\n",
        "    def isbetter(self, new_res, old_res):\n",
        "        if self.better == 'large':\n",
        "            return new_res > old_res\n",
        "        if self.better == 'small':\n",
        "            return new_res < old_res\n",
        "\n",
        "    def update(self, new_res, ep):\n",
        "        if self.isbetter(new_res, self.best_res):\n",
        "            self.best_res = new_res\n",
        "            self.best_ep = ep\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return \"best_res: {}\\t best_ep: {}\".format(self.best_res, self.best_ep)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return self.__str__()\n",
        "\n",
        "    def summary(self) -> dict:\n",
        "        return {\n",
        "            'best_res': self.best_res,\n",
        "            'best_ep': self.best_ep,\n",
        "        }\n",
        "\n",
        "\n",
        "class BestMetricHolder():\n",
        "    def __init__(self, init_res=0.0, better='large', use_ema=False) -> None:\n",
        "        self.best_all = BestMetricSingle(init_res, better)\n",
        "        self.use_ema = use_ema\n",
        "        if use_ema:\n",
        "            self.best_ema = BestMetricSingle(init_res, better)\n",
        "            self.best_regular = BestMetricSingle(init_res, better)\n",
        "\n",
        "\n",
        "    def update(self, new_res, epoch, is_ema=False):\n",
        "        \"\"\"\n",
        "        return if the results is the best.\n",
        "        \"\"\"\n",
        "        if not self.use_ema:\n",
        "            return self.best_all.update(new_res, epoch)\n",
        "        else:\n",
        "            if is_ema:\n",
        "                self.best_ema.update(new_res, epoch)\n",
        "                return self.best_all.update(new_res, epoch)\n",
        "            else:\n",
        "                self.best_regular.update(new_res, epoch)\n",
        "                return self.best_all.update(new_res, epoch)\n",
        "\n",
        "    def summary(self):\n",
        "        if not self.use_ema:\n",
        "            return self.best_all.summary()\n",
        "\n",
        "        res = {}\n",
        "        res.update({f'all_{k}':v for k,v in self.best_all.summary().items()})\n",
        "        res.update({f'regular_{k}':v for k,v in self.best_regular.summary().items()})\n",
        "        res.update({f'ema_{k}':v for k,v in self.best_ema.summary().items()})\n",
        "        return res\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return json.dumps(self.summary(), indent=2)\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return self.__repr__()\n",
        ""
      ],
      "metadata": {
        "id": "NQJs7Jt0LGlj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
        "\"\"\"\n",
        "COCO evaluator that works in distributed mode.\n",
        "\n",
        "Mostly copy-paste from https://github.com/pytorch/vision/blob/edfd5a7/references/detection/coco_eval.py\n",
        "The difference is that there is less copy-pasting from pycocotools\n",
        "in the end of the file, as python3 can suppress prints with contextlib\n",
        "\"\"\"\n",
        "import os\n",
        "import contextlib\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools.coco import COCO\n",
        "import pycocotools.mask as mask_util\n",
        "\n",
        "\n",
        "\n",
        "class CocoEvaluator(object):\n",
        "    def __init__(self, coco_gt, iou_types, useCats=True):\n",
        "        assert isinstance(iou_types, (list, tuple))\n",
        "        coco_gt = copy.deepcopy(coco_gt)\n",
        "        self.coco_gt = coco_gt\n",
        "\n",
        "        self.iou_types = iou_types\n",
        "        self.coco_eval = {}\n",
        "        for iou_type in iou_types:\n",
        "            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n",
        "            self.coco_eval[iou_type].useCats = useCats\n",
        "\n",
        "        self.img_ids = []\n",
        "        self.eval_imgs = {k: [] for k in iou_types}\n",
        "        self.useCats = useCats\n",
        "\n",
        "    def update(self, predictions):\n",
        "        img_ids = list(np.unique(list(predictions.keys())))\n",
        "        self.img_ids.extend(img_ids)\n",
        "\n",
        "        for iou_type in self.iou_types:\n",
        "            results = self.prepare(predictions, iou_type)\n",
        "\n",
        "            # suppress pycocotools prints\n",
        "            with open(os.devnull, 'w') as devnull:\n",
        "                with contextlib.redirect_stdout(devnull):\n",
        "                    coco_dt = COCO.loadRes(self.coco_gt, results) if results else COCO()\n",
        "            coco_eval = self.coco_eval[iou_type]\n",
        "\n",
        "            coco_eval.cocoDt = coco_dt\n",
        "            coco_eval.params.imgIds = list(img_ids)\n",
        "            coco_eval.params.useCats = self.useCats\n",
        "            img_ids, eval_imgs = evaluate(coco_eval)\n",
        "\n",
        "            self.eval_imgs[iou_type].append(eval_imgs)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for iou_type in self.iou_types:\n",
        "            self.eval_imgs[iou_type] = np.concatenate(self.eval_imgs[iou_type], 2)\n",
        "            create_common_coco_eval(self.coco_eval[iou_type], self.img_ids, self.eval_imgs[iou_type])\n",
        "\n",
        "    def accumulate(self):\n",
        "        for coco_eval in self.coco_eval.values():\n",
        "            coco_eval.accumulate()\n",
        "\n",
        "    def summarize(self):\n",
        "        for iou_type, coco_eval in self.coco_eval.items():\n",
        "            print(\"IoU metric: {}\".format(iou_type))\n",
        "            coco_eval.summarize()\n",
        "\n",
        "    def prepare(self, predictions, iou_type):\n",
        "        if iou_type == \"bbox\":\n",
        "            return self.prepare_for_coco_detection(predictions)\n",
        "        elif iou_type == \"segm\":\n",
        "            return self.prepare_for_coco_segmentation(predictions)\n",
        "        elif iou_type == \"keypoints\":\n",
        "            return self.prepare_for_coco_keypoint(predictions)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown iou type {}\".format(iou_type))\n",
        "\n",
        "    def prepare_for_coco_detection(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            if not isinstance(prediction[\"scores\"], list):\n",
        "                scores = prediction[\"scores\"].tolist()\n",
        "            else:\n",
        "                scores = prediction[\"scores\"]\n",
        "            if not isinstance(prediction[\"labels\"], list):\n",
        "                labels = prediction[\"labels\"].tolist()\n",
        "            else:\n",
        "                labels = prediction[\"labels\"]\n",
        "\n",
        "\n",
        "            try:\n",
        "                coco_results.extend(\n",
        "                    [\n",
        "                        {\n",
        "                            \"image_id\": original_id,\n",
        "                            \"category_id\": labels[k],\n",
        "                            \"bbox\": box,\n",
        "                            \"score\": scores[k],\n",
        "                        }\n",
        "                        for k, box in enumerate(boxes)\n",
        "                    ]\n",
        "                )\n",
        "            except:\n",
        "                import ipdb; ipdb.set_trace()\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_segmentation(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            scores = prediction[\"scores\"]\n",
        "            labels = prediction[\"labels\"]\n",
        "            masks = prediction[\"masks\"]\n",
        "\n",
        "            masks = masks > 0.5\n",
        "\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            rles = [\n",
        "                mask_util.encode(np.array(mask[0, :, :, np.newaxis], dtype=np.uint8, order=\"F\"))[0]\n",
        "                for mask in masks\n",
        "            ]\n",
        "            for rle in rles:\n",
        "                rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"segmentation\": rle,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, rle in enumerate(rles)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_keypoint(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "            keypoints = prediction[\"keypoints\"]\n",
        "            keypoints = keypoints.flatten(start_dim=1).tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        'keypoints': keypoint,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, keypoint in enumerate(keypoints)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "\n",
        "def convert_to_xywh(boxes):\n",
        "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
        "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
        "\n",
        "\n",
        "def merge(img_ids, eval_imgs):\n",
        "    all_img_ids = all_gather(img_ids)\n",
        "    all_eval_imgs = all_gather(eval_imgs)\n",
        "\n",
        "    merged_img_ids = []\n",
        "    for p in all_img_ids:\n",
        "        merged_img_ids.extend(p)\n",
        "\n",
        "    merged_eval_imgs = []\n",
        "    for p in all_eval_imgs:\n",
        "        merged_eval_imgs.append(p)\n",
        "\n",
        "    merged_img_ids = np.array(merged_img_ids)\n",
        "    merged_eval_imgs = np.concatenate(merged_eval_imgs, 2)\n",
        "\n",
        "    # keep only unique (and in sorted order) images\n",
        "    merged_img_ids, idx = np.unique(merged_img_ids, return_index=True)\n",
        "    merged_eval_imgs = merged_eval_imgs[..., idx]\n",
        "\n",
        "    return merged_img_ids, merged_eval_imgs\n",
        "\n",
        "\n",
        "def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n",
        "    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n",
        "    img_ids = list(img_ids)\n",
        "    eval_imgs = list(eval_imgs.flatten())\n",
        "\n",
        "    coco_eval.evalImgs = eval_imgs\n",
        "    coco_eval.params.imgIds = img_ids\n",
        "    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n",
        "\n",
        "\n",
        "#################################################################\n",
        "# From pycocotools, just removed the prints and fixed\n",
        "# a Python3 bug about unicode not defined\n",
        "#################################################################\n",
        "\n",
        "\n",
        "def evaluate(self):\n",
        "    '''\n",
        "    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n",
        "    :return: None\n",
        "    '''\n",
        "    p = self.params\n",
        "    # add backward compatibility if useSegm is specified in params\n",
        "    if p.useSegm is not None:\n",
        "        p.iouType = 'segm' if p.useSegm == 1 else 'bbox'\n",
        "        print('useSegm (deprecated) is not None. Running {} evaluation'.format(p.iouType))\n",
        "    p.imgIds = list(np.unique(p.imgIds))\n",
        "    if p.useCats:\n",
        "        p.catIds = list(np.unique(p.catIds))\n",
        "    p.maxDets = sorted(p.maxDets)\n",
        "    self.params = p\n",
        "\n",
        "    self._prepare()\n",
        "    # loop through images, area range, max detection number\n",
        "    catIds = p.catIds if p.useCats else [-1]\n",
        "\n",
        "    if p.iouType == 'segm' or p.iouType == 'bbox':\n",
        "        computeIoU = self.computeIoU\n",
        "    elif p.iouType == 'keypoints':\n",
        "        computeIoU = self.computeOks\n",
        "    self.ious = {\n",
        "        (imgId, catId): computeIoU(imgId, catId)\n",
        "        for imgId in p.imgIds\n",
        "        for catId in catIds}\n",
        "\n",
        "    evaluateImg = self.evaluateImg\n",
        "    maxDet = p.maxDets[-1]\n",
        "    evalImgs = [\n",
        "        evaluateImg(imgId, catId, areaRng, maxDet)\n",
        "        for catId in catIds\n",
        "        for areaRng in p.areaRng\n",
        "        for imgId in p.imgIds\n",
        "    ]\n",
        "    # this is NOT in the pycocotools code, but could be done outside\n",
        "    evalImgs = np.asarray(evalImgs).reshape(len(catIds), len(p.areaRng), len(p.imgIds))\n",
        "    self._paramsEval = copy.deepcopy(self.params)\n",
        "\n",
        "    return p.imgIds, evalImgs\n",
        "\n",
        "#################################################################\n",
        "# end of straight copy from pycocotools, just removing the prints\n",
        "#################################################################\n"
      ],
      "metadata": {
        "id": "a9U-AT02LU-c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "try:\n",
        "    from panopticapi.evaluation import pq_compute\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "\n",
        "class PanopticEvaluator(object):\n",
        "    def __init__(self, ann_file, ann_folder, output_dir=\"panoptic_eval\"):\n",
        "        self.gt_json = ann_file\n",
        "        self.gt_folder = ann_folder\n",
        "        if utils.is_main_process():\n",
        "            if not os.path.exists(output_dir):\n",
        "                os.mkdir(output_dir)\n",
        "        self.output_dir = output_dir\n",
        "        self.predictions = []\n",
        "\n",
        "    def update(self, predictions):\n",
        "        for p in predictions:\n",
        "            with open(os.path.join(self.output_dir, p[\"file_name\"]), \"wb\") as f:\n",
        "                f.write(p.pop(\"png_string\"))\n",
        "\n",
        "        self.predictions += predictions\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        all_predictions = utils.all_gather(self.predictions)\n",
        "        merged_predictions = []\n",
        "        for p in all_predictions:\n",
        "            merged_predictions += p\n",
        "        self.predictions = merged_predictions\n",
        "\n",
        "    def summarize(self):\n",
        "        if utils.is_main_process():\n",
        "            json_data = {\"annotations\": self.predictions}\n",
        "            predictions_json = os.path.join(self.output_dir, \"predictions.json\")\n",
        "            with open(predictions_json, \"w\") as f:\n",
        "                f.write(json.dumps(json_data))\n",
        "            return pq_compute(self.gt_json, predictions_json, gt_folder=self.gt_folder, pred_folder=self.output_dir)\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "s8Be26QXLeL4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
        "\"\"\"\n",
        "Train and eval functions used in main.py\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "from typing import Iterable\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n",
        "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
        "                    device: torch.device, epoch: int, max_norm: float = 0,\n",
        "                    wo_class_error=False, lr_scheduler=None, args=None, logger=None, ema_m=None):\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=args.amp)\n",
        "\n",
        "    try:\n",
        "        need_tgt_for_training = args.use_dn\n",
        "    except:\n",
        "        need_tgt_for_training = False\n",
        "\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    if not wo_class_error:\n",
        "        metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
        "    header = 'Epoch: [{}]'.format(epoch)\n",
        "    print_freq = 10\n",
        "\n",
        "    _cnt = 0\n",
        "    for samples, targets in metric_logger.log_every(data_loader, print_freq, header, logger=logger):\n",
        "\n",
        "        samples = samples.to(device)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=args.amp):\n",
        "            if need_tgt_for_training:\n",
        "                outputs = model(samples, targets)\n",
        "            else:\n",
        "                outputs = model(samples)\n",
        "\n",
        "            loss_dict = criterion(outputs, targets)\n",
        "            weight_dict = criterion.weight_dict\n",
        "\n",
        "            losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
        "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
        "                                      for k, v in loss_dict_reduced.items()}\n",
        "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
        "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
        "        losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())\n",
        "\n",
        "        loss_value = losses_reduced_scaled.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "\n",
        "\n",
        "        # amp backward function\n",
        "        if args.amp:\n",
        "            optimizer.zero_grad()\n",
        "            scaler.scale(losses).backward()\n",
        "            if max_norm > 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            # original backward function\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            if max_norm > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "        if args.onecyclelr:\n",
        "            lr_scheduler.step()\n",
        "        if args.use_ema:\n",
        "            if epoch >= args.ema_epoch:\n",
        "                ema_m.update(model)\n",
        "\n",
        "        metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n",
        "        if 'class_error' in loss_dict_reduced:\n",
        "            metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "        _cnt += 1\n",
        "        if args.debug:\n",
        "            if _cnt % 15 == 0:\n",
        "                print(\"BREAK!\"*5)\n",
        "                break\n",
        "\n",
        "    if getattr(criterion, 'loss_weight_decay', False):\n",
        "        criterion.loss_weight_decay(epoch=epoch)\n",
        "    if getattr(criterion, 'tuning_matching', False):\n",
        "        criterion.tuning_matching(epoch)\n",
        "\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    resstat = {k: meter.global_avg for k, meter in metric_logger.meters.items() if meter.count > 0}\n",
        "    if getattr(criterion, 'loss_weight_decay', False):\n",
        "        resstat.update({f'weight_{k}': v for k,v in criterion.weight_dict.items()})\n",
        "    return resstat\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, criterion, postprocessors, data_loader, base_ds, device, output_dir, wo_class_error=False, args=None, logger=None):\n",
        "    try:\n",
        "        need_tgt_for_training = args.use_dn\n",
        "    except:\n",
        "        need_tgt_for_training = False\n",
        "\n",
        "    model.eval()\n",
        "    criterion.eval()\n",
        "\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    if not wo_class_error:\n",
        "        metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
        "    header = 'Test:'\n",
        "\n",
        "    iou_types = tuple(k for k in ('segm', 'bbox') if k in postprocessors.keys())\n",
        "    useCats = True\n",
        "    try:\n",
        "        useCats = args.useCats\n",
        "    except:\n",
        "        useCats = True\n",
        "    if not useCats:\n",
        "        print(\"useCats: {} !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\".format(useCats))\n",
        "    coco_evaluator = CocoEvaluator(base_ds, iou_types, useCats=useCats)\n",
        "    # coco_evaluator.coco_eval[iou_types[0]].params.iouThrs = [0, 0.1, 0.5, 0.75]\n",
        "\n",
        "    panoptic_evaluator = None\n",
        "    if 'panoptic' in postprocessors.keys():\n",
        "        panoptic_evaluator = PanopticEvaluator(\n",
        "            data_loader.dataset.ann_file,\n",
        "            data_loader.dataset.ann_folder,\n",
        "            output_dir=os.path.join(output_dir, \"panoptic_eval\"),\n",
        "        )\n",
        "\n",
        "    _cnt = 0\n",
        "    output_state_dict = {} # for debug only\n",
        "    for samples, targets in metric_logger.log_every(data_loader, 10, header, logger=logger):\n",
        "        samples = samples.to(device)\n",
        "\n",
        "        # targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        targets = [{k: to_device(v, device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=args.amp):\n",
        "            if need_tgt_for_training:\n",
        "                outputs = model(samples, targets)\n",
        "            else:\n",
        "                outputs = model(samples)\n",
        "            # outputs = model(samples)\n",
        "\n",
        "            loss_dict = criterion(outputs, targets)\n",
        "        weight_dict = criterion.weight_dict\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
        "        loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
        "                                    for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
        "        loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
        "                                      for k, v in loss_dict_reduced.items()}\n",
        "        metric_logger.update(loss=sum(loss_dict_reduced_scaled.values()),\n",
        "                             **loss_dict_reduced_scaled,\n",
        "                             **loss_dict_reduced_unscaled)\n",
        "        if 'class_error' in loss_dict_reduced:\n",
        "            metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
        "\n",
        "        orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n",
        "        results = postprocessors['bbox'](outputs, orig_target_sizes)\n",
        "        # [scores: [100], labels: [100], boxes: [100, 4]] x B\n",
        "        if 'segm' in postprocessors.keys():\n",
        "            target_sizes = torch.stack([t[\"size\"] for t in targets], dim=0)\n",
        "            results = postprocessors['segm'](results, outputs, orig_target_sizes, target_sizes)\n",
        "        res = {target['image_id'].item(): output for target, output in zip(targets, results)}\n",
        "\n",
        "        if coco_evaluator is not None:\n",
        "            coco_evaluator.update(res)\n",
        "\n",
        "        if panoptic_evaluator is not None:\n",
        "            res_pano = postprocessors[\"panoptic\"](outputs, target_sizes, orig_target_sizes)\n",
        "            for i, target in enumerate(targets):\n",
        "                image_id = target[\"image_id\"].item()\n",
        "                file_name = f\"{image_id:012d}.png\"\n",
        "                res_pano[i][\"image_id\"] = image_id\n",
        "                res_pano[i][\"file_name\"] = file_name\n",
        "\n",
        "            panoptic_evaluator.update(res_pano)\n",
        "\n",
        "        if args.save_results:\n",
        "            # res_score = outputs['res_score']\n",
        "            # res_label = outputs['res_label']\n",
        "            # res_bbox = outputs['res_bbox']\n",
        "            # res_idx = outputs['res_idx']\n",
        "\n",
        "\n",
        "            for i, (tgt, res, outbbox) in enumerate(zip(targets, results, outputs['pred_boxes'])):\n",
        "                \"\"\"\n",
        "                pred vars:\n",
        "                    K: number of bbox pred\n",
        "                    score: Tensor(K),\n",
        "                    label: list(len: K),\n",
        "                    bbox: Tensor(K, 4)\n",
        "                    idx: list(len: K)\n",
        "                tgt: dict.\n",
        "\n",
        "                \"\"\"\n",
        "                # compare gt and res (after postprocess)\n",
        "                gt_bbox = tgt['boxes']\n",
        "                gt_label = tgt['labels']\n",
        "                gt_info = torch.cat((gt_bbox, gt_label.unsqueeze(-1)), 1)\n",
        "\n",
        "                # img_h, img_w = tgt['orig_size'].unbind()\n",
        "                # scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=0)\n",
        "                # _res_bbox = res['boxes'] / scale_fct\n",
        "                _res_bbox = outbbox\n",
        "                _res_prob = res['scores']\n",
        "                _res_label = res['labels']\n",
        "                res_info = torch.cat((_res_bbox, _res_prob.unsqueeze(-1), _res_label.unsqueeze(-1)), 1)\n",
        "                # import ipdb;ipdb.set_trace()\n",
        "\n",
        "                if 'gt_info' not in output_state_dict:\n",
        "                    output_state_dict['gt_info'] = []\n",
        "                output_state_dict['gt_info'].append(gt_info.cpu())\n",
        "\n",
        "                if 'res_info' not in output_state_dict:\n",
        "                    output_state_dict['res_info'] = []\n",
        "                output_state_dict['res_info'].append(res_info.cpu())\n",
        "\n",
        "            # # for debug only\n",
        "            # import random\n",
        "            # if random.random() > 0.7:\n",
        "            #     print(\"Now let's break\")\n",
        "            #     break\n",
        "\n",
        "        _cnt += 1\n",
        "        if args.debug:\n",
        "            if _cnt % 15 == 0:\n",
        "                print(\"BREAK!\"*5)\n",
        "                break\n",
        "\n",
        "    if args.save_results:\n",
        "        import os.path as osp\n",
        "\n",
        "        # output_state_dict['gt_info'] = torch.cat(output_state_dict['gt_info'])\n",
        "        # output_state_dict['res_info'] = torch.cat(output_state_dict['res_info'])\n",
        "        savepath = osp.join(args.output_dir, 'results-{}.pkl'.format(utils.get_rank()))\n",
        "        print(\"Saving res to {}\".format(savepath))\n",
        "        torch.save(output_state_dict, savepath)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    if coco_evaluator is not None:\n",
        "        coco_evaluator.synchronize_between_processes()\n",
        "    if panoptic_evaluator is not None:\n",
        "        panoptic_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # accumulate predictions from all images\n",
        "    if coco_evaluator is not None:\n",
        "        coco_evaluator.accumulate()\n",
        "        coco_evaluator.summarize()\n",
        "\n",
        "    panoptic_res = None\n",
        "    if panoptic_evaluator is not None:\n",
        "        panoptic_res = panoptic_evaluator.summarize()\n",
        "    stats = {k: meter.global_avg for k, meter in metric_logger.meters.items() if meter.count > 0}\n",
        "    if coco_evaluator is not None:\n",
        "        if 'bbox' in postprocessors.keys():\n",
        "            stats['coco_eval_bbox'] = coco_evaluator.coco_eval['bbox'].stats.tolist()\n",
        "        if 'segm' in postprocessors.keys():\n",
        "            stats['coco_eval_masks'] = coco_evaluator.coco_eval['segm'].stats.tolist()\n",
        "    if panoptic_res is not None:\n",
        "        stats['PQ_all'] = panoptic_res[\"All\"]\n",
        "        stats['PQ_th'] = panoptic_res[\"Things\"]\n",
        "        stats['PQ_st'] = panoptic_res[\"Stuff\"]\n",
        "\n",
        "\n",
        "\n",
        "    return stats, coco_evaluator\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, criterion, postprocessors, data_loader, base_ds, device, output_dir, wo_class_error=False, args=None, logger=None):\n",
        "    model.eval()\n",
        "    criterion.eval()\n",
        "\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    # if not wo_class_error:\n",
        "    #     metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n",
        "    header = 'Test:'\n",
        "\n",
        "    iou_types = tuple(k for k in ('segm', 'bbox') if k in postprocessors.keys())\n",
        "    # coco_evaluator = CocoEvaluator(base_ds, iou_types)\n",
        "    # coco_evaluator.coco_eval[iou_types[0]].params.iouThrs = [0, 0.1, 0.5, 0.75]\n",
        "\n",
        "    panoptic_evaluator = None\n",
        "    if 'panoptic' in postprocessors.keys():\n",
        "        panoptic_evaluator = PanopticEvaluator(\n",
        "            data_loader.dataset.ann_file,\n",
        "            data_loader.dataset.ann_folder,\n",
        "            output_dir=os.path.join(output_dir, \"panoptic_eval\"),\n",
        "        )\n",
        "\n",
        "    final_res = []\n",
        "    for samples, targets in metric_logger.log_every(data_loader, 10, header, logger=logger):\n",
        "        samples = samples.to(device)\n",
        "\n",
        "        # targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        targets = [{k: to_device(v, device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        outputs = model(samples)\n",
        "        # loss_dict = criterion(outputs, targets)\n",
        "        # weight_dict = criterion.weight_dict\n",
        "\n",
        "        # # reduce losses over all GPUs for logging purposes\n",
        "        # loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
        "        # loss_dict_reduced_scaled = {k: v * weight_dict[k]\n",
        "        #                             for k, v in loss_dict_reduced.items() if k in weight_dict}\n",
        "        # loss_dict_reduced_unscaled = {f'{k}_unscaled': v\n",
        "        #                               for k, v in loss_dict_reduced.items()}\n",
        "        # metric_logger.update(loss=sum(loss_dict_reduced_scaled.values()),\n",
        "        #                      **loss_dict_reduced_scaled,\n",
        "        #                      **loss_dict_reduced_unscaled)\n",
        "        # if 'class_error' in loss_dict_reduced:\n",
        "        #     metric_logger.update(class_error=loss_dict_reduced['class_error'])\n",
        "\n",
        "        orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n",
        "        results = postprocessors['bbox'](outputs, orig_target_sizes, not_to_xyxy=True)\n",
        "        # [scores: [100], labels: [100], boxes: [100, 4]] x B\n",
        "        if 'segm' in postprocessors.keys():\n",
        "            target_sizes = torch.stack([t[\"size\"] for t in targets], dim=0)\n",
        "            results = postprocessors['segm'](results, outputs, orig_target_sizes, target_sizes)\n",
        "        res = {target['image_id'].item(): output for target, output in zip(targets, results)}\n",
        "        for image_id, outputs in res.items():\n",
        "            _scores = outputs['scores'].tolist()\n",
        "            _labels = outputs['labels'].tolist()\n",
        "            _boxes = outputs['boxes'].tolist()\n",
        "            for s, l, b in zip(_scores, _labels, _boxes):\n",
        "                assert isinstance(l, int)\n",
        "                itemdict = {\n",
        "                        \"image_id\": int(image_id),\n",
        "                        \"category_id\": l,\n",
        "                        \"bbox\": b,\n",
        "                        \"score\": s,\n",
        "                        }\n",
        "                final_res.append(itemdict)\n",
        "\n",
        "    if args.output_dir:\n",
        "        import json\n",
        "        with open(args.output_dir + f'/results{args.rank}.json', 'w') as f:\n",
        "            json.dump(final_res, f)\n",
        "\n",
        "    return final_res\n"
      ],
      "metadata": {
        "id": "KQUX1iwpK_N7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSES = [\n",
        "    'N/A', 'person'\n",
        "]\n",
        "\n",
        "coco_idx_to_label = {idx: label for idx, label in enumerate(CLASSES)}\n",
        "\n",
        "# colors for visualization\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]"
      ],
      "metadata": {
        "id": "t4GEi1_RLyop"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "        Returns:\n",
        "            Tensor: Normalized image.\n",
        "        \"\"\"\n",
        "        output_tensor = []\n",
        "        for t, m, s in zip(tensor, self.mean, self.std):\n",
        "            output_tensor.append(t.mul(s).add(m))\n",
        "            # t.mul_(s).add_(m)\n",
        "            # The normalize code -> t.sub_(m).div_(s)\n",
        "        return torch.stack(output_tensor, dim=0)\n",
        "\n",
        "unnorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "\n",
        "# for output bounding box post-processing\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=1)\n",
        "\n",
        "def rescale_bboxes(out_bbox, size):\n",
        "    img_w, img_h = size\n",
        "    b = box_cxcywh_to_xyxy(out_bbox)\n",
        "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "    return b\n",
        "\n",
        "def plot_results(img, labels, boxes, mask=None):\n",
        "    h, w = img.shape[1:]\n",
        "    if mask != None:\n",
        "        # width\n",
        "        if torch.where(mask[0])[0].shape[0] > 0:\n",
        "            mask_w = torch.where(mask[0])[0][0]\n",
        "            w = min(w, mask_w)\n",
        "        if torch.where(mask[:, 0])[0].shape[0]:\n",
        "            mask_h = torch.where(mask[:, 0])[0][0]\n",
        "            h = min(h, mask_h)\n",
        "\n",
        "    boxes = rescale_bboxes(boxes, (w, h))\n",
        "    plt.figure(figsize=(16,10))\n",
        "    unimage = unnorm(img)\n",
        "    #image = (unimage*256).to(torch.uint8)\n",
        "    image = unimage\n",
        "    pil_img = torchvision.transforms.functional.to_pil_image(image)\n",
        "    plt.imshow(pil_img)\n",
        "\n",
        "    ax = plt.gca()\n",
        "    colors = COLORS * 100\n",
        "    for label, (xmin, ymin, xmax, ymax), c in zip(labels, boxes.tolist(), colors):\n",
        "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                   fill=False, color=c, linewidth=3))\n",
        "        text = f'{CLASSES[label]}'\n",
        "        ax.text(xmin, ymin, text, fontsize=15,\n",
        "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qmc0MaNwLymk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "args = Namespace(config_file='/content/DINO/config/DINO/DINO_4scale.py', options={'dn_scalar': 100, 'embed_init_tgt': True, 'dn_label_coef': 1.0, 'dn_bbox_coef': 1.0, 'use_ema': False, 'dn_box_noise_scale': 1.0}, dataset_file='coco', coco_path='/home/maksimgaiduk/repos/datasets/coco_original', coco_panoptic_path=None, remove_difficult=False, fix_size=False, output_dir='logs/DINO/R50-MS4', note='', device='cuda', seed=42, resume='', pretrain_model_path=None, finetune_ignore=None, start_epoch=0, eval=False, num_workers=10, test=False, debug=False, find_unused_params=False, save_results=False, save_log=False, world_size=1, dist_url='env://', rank=0, local_rank=0, amp=False, distributed=False, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_max_size=1333, data_aug_scales2_resize=[400, 500, 600], data_aug_scales2_crop=[384, 600], data_aug_scale_overlap=None, num_classes=91, lr=0.0001, param_dict_type='default', lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, ddetr_lr_param=False, batch_size=2, weight_decay=0.0001, epochs=12, lr_drop=11, save_checkpoint_interval=1, clip_max_norm=0.1, onecyclelr=False, multi_step_lr=False, lr_drop_list=[33, 45], modelname='dino', frozen_weights=None, backbone='resnet50', use_checkpoint=False, dilation=False, position_embedding='sine', pe_temperatureH=20, pe_temperatureW=20, return_interm_indices=[1, 2, 3], backbone_freeze_keywords=None, enc_layers=6, dec_layers=6, unic_layers=0, pre_norm=False, dim_feedforward=2048, hidden_dim=256, dropout=0.0, nheads=8, num_queries=900, query_dim=4, num_patterns=0, pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, random_refpoints_xy=False, fix_refpoints_hw=-1, dabdetr_yolo_like_anchor_update=False, dabdetr_deformable_encoder=False, dabdetr_deformable_decoder=False, use_deformable_box_attn=False, box_attn_type='roi_align', dec_layer_number=None, num_feature_levels=4, enc_n_points=4, dec_n_points=4, decoder_layer_noise=False, dln_xy_noise=0.2, dln_hw_noise=0.2, add_channel_attention=False, add_pos_value=False, two_stage_type='standard', two_stage_pat_embed=0, two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_learn_wh=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, num_select=300, transformer_activation='relu', batch_norm_type='FrozenBatchNorm2d', masks=False, aux_loss=True, set_cost_class=2.0, set_cost_bbox=5.0, set_cost_giou=2.0, cls_loss_coef=1.0, mask_loss_coef=1.0, dice_loss_coef=1.0, bbox_loss_coef=5.0, giou_loss_coef=2.0, enc_loss_coef=1.0, interm_loss_coef=1.0, no_interm_box_loss=False, focal_alpha=0.25, decoder_sa_type='sa', matcher_type='HungarianMatcher', decoder_module_seq=['sa', 'ca', 'ffn'], nms_iou_threshold=-1, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, use_dn=True, dn_number=100, dn_box_noise_scale=1.0, dn_label_noise_ratio=0.5, embed_init_tgt=True, dn_labelbook_size=91, match_unstable_error=True, use_ema=False, ema_decay=0.9997, ema_epoch=0, use_detached_boxes_dec_out=False, dn_scalar=100, dn_label_coef=1.0, dn_bbox_coef=1.0)"
      ],
      "metadata": {
        "id": "Jvzq8iejLyZN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seed = args.seed + utils.get_rank()\n",
        "# torch.manual_seed(seed)\n",
        "# np.random.seed(seed)\n",
        "# random.seed(seed)"
      ],
      "metadata": {
        "id": "gxfoQllsMfTg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6RWDQ3shMfNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = '/content/Pedestrian_dataset_for_internship_assignment'\n",
        "annotations_file = '/content/random_sample_mavi_2_gt.json'\n",
        "output_dir = '/content/Pedestrian_dataset_for_internship_assignment_split'\n",
        "\n",
        "num_train_samples = 160\n",
        "num_val_samples = 40\n",
        "\n",
        "\n",
        "os.makedirs(os.path.join(output_dir, 'train'), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_dir, 'val'), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_dir, 'annotations'), exist_ok=True)\n",
        "\n",
        "\n",
        "shutil.copy(annotations_file, os.path.join(output_dir, 'annotations/instances_val2017.json'))\n",
        "\n",
        "dataset = CocoDetection(root=images_dir, annFile=annotations_file)\n",
        "\n",
        "num_samples = len(dataset)\n",
        "if num_samples < 200:\n",
        "    num_train_samples = int(num_samples * 0.8)\n",
        "    num_val_samples = num_samples - num_train_samples\n",
        "train_dataset, val_dataset = random_split(dataset, [num_train_samples, num_val_samples])\n",
        "\n",
        "def save_images(dataset, folder_name):\n",
        "    for i, (img, _) in enumerate(dataset):\n",
        "        img_info = dataset.dataset.coco.loadImgs(dataset.dataset.ids[dataset.indices[i]])[0]\n",
        "        img_name = img_info['file_name']\n",
        "        img_path = os.path.join(images_dir, img_name)\n",
        "\n",
        "        dest_path = os.path.join(output_dir, folder_name, img_name)\n",
        "        shutil.copy(img_path, dest_path)\n",
        "\n",
        "\n",
        "save_images(train_dataset, 'train')\n",
        "save_images(val_dataset, 'val')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BR4Md7n81yA",
        "outputId": "eb32b043-aa52-4337-c742-cf0fbe2d86f3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Dataset organized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/IDEA-Research/DINO.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-E0uKi5XWXv",
        "outputId": "1ddf4a83-beef-4dbb-c6c0-45175dd22bf9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DINO'...\n",
            "remote: Enumerating objects: 442, done.\u001b[K\n",
            "remote: Counting objects: 100% (191/191), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 442 (delta 136), reused 96 (delta 96), pack-reused 251 (from 1)\u001b[K\n",
            "Receiving objects: 100% (442/442), 13.43 MiB | 16.40 MiB/s, done.\n",
            "Resolving deltas: 100% (191/191), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/DINO/models/dino/ops\n",
        "!python /content/DINO/models/dino/ops/setup.py build install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16BXdFU-XgHu",
        "outputId": "ad2bd130-0ed7-4074-a749-60fab5f37b5a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running build\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:497: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:416: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:426: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "building 'MultiScaleDeformableAttention' extension\n",
            "creating build/temp.linux-x86_64-cpython-310/content/DINO/models/dino/ops/src/cpu\n",
            "creating build/temp.linux-x86_64-cpython-310/content/DINO/models/dino/ops/src/cuda\n",
            "x86_64-linux-gnu-g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -DWITH_CUDA -I/content/DINO/models/dino/ops/src -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c /content/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.cpp -o build/temp.linux-x86_64-cpython-310/content/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/DINO/models/dino/ops/src -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c /content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu -o build/temp.linux-x86_64-cpython-310/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(261)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_im2col_cuda(cudaStream_t, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 64 of /content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(762)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 134 of /content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(872)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 134 of /content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(331)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 134 of /content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(436)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 134 of /content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(544)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 134 of /content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(649)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 134 of /content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kat::Tensor ms_deform_attn_cuda_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:34:61:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   34 |     AT_ASSERTM(value.type().is_cuda(), \"value must\u001b[01;35m\u001b[K be a CUDA t\u001b[m\u001b[Kensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:35:70:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   35 |     AT_ASSERTM(spatial_shapes.type().is_cuda(), \"s\u001b[01;35m\u001b[Kpatial_shapes must be\u001b[m\u001b[K a CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:36:73:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   36 |     AT_ASSERTM(level_start_index.type().is_cuda(),\u001b[01;35m\u001b[K \"level_start_index must\u001b[m\u001b[K be a CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:37:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   37 |     AT_ASSERTM(sampling_loc.type().is_cuda(), \"sam\u001b[01;35m\u001b[Kpling_loc must be a\u001b[m\u001b[K CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:38:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   38 |     AT_ASSERTM(attn_weight.type().is_cuda(), \"attn\u001b[01;35m\u001b[K_weight must be a \u001b[m\u001b[KCUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_\u001b[01;35m\u001b[KTYPES(value.ty\u001b[m\u001b[Kpe(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:163:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K         \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:109:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1040:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1126:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1169:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1202:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1285:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1443:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2300:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2386:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2429:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2461:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2543:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2700:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::vector<at::Tensor> ms_deform_attn_cuda_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:100:61:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  100 |     AT_ASSERTM(value.type().is_cuda(), \"value must\u001b[01;35m\u001b[K be a CUDA t\u001b[m\u001b[Kensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:101:70:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  101 |     AT_ASSERTM(spatial_shapes.type().is_cuda(), \"s\u001b[01;35m\u001b[Kpatial_shapes must be\u001b[m\u001b[K a CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:102:73:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  102 |     AT_ASSERTM(level_start_index.type().is_cuda(),\u001b[01;35m\u001b[K \"level_start_index must\u001b[m\u001b[K be a CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:103:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  103 |     AT_ASSERTM(sampling_loc.type().is_cuda(), \"sam\u001b[01;35m\u001b[Kpling_loc must be a\u001b[m\u001b[K CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:104:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  104 |     AT_ASSERTM(attn_weight.type().is_cuda(), \"attn\u001b[01;35m\u001b[K_weight must be a \u001b[m\u001b[KCUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:105:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  105 |     AT_ASSERTM(grad_output.type().is_cuda(), \"grad\u001b[01;35m\u001b[K_output must be a \u001b[m\u001b[KCUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_\u001b[01;35m\u001b[KTYPES(value.ty\u001b[m\u001b[Kpe(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:164:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K         \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:109:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1050:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1076:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1162:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1205:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1238:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1321:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1482:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1566:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1654:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2572:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2597:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2683:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2726:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2758:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2840:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:3000:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:3083:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:3170:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -DWITH_CUDA -I/content/DINO/models/dino/ops/src -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c /content/DINO/models/dino/ops/src/vision.cpp -o build/temp.linux-x86_64-cpython-310/content/DINO/models/dino/ops/src/vision.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "In file included from \u001b[01m\u001b[K/content/DINO/models/dino/ops/src/vision.cpp:11\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/ms_deform_attn.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kat::Tensor ms_deform_attn_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/ms_deform_attn.h:29:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   29 |     if (\u001b[01;35m\u001b[Kvalue.type()\u001b[m\u001b[K.is_cuda())\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/DINO/models/dino/ops/src/ms_deform_attn.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/DINO/models/dino/ops/src/vision.cpp:11\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/content/DINO/models/dino/ops/src/vision.cpp:11\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/ms_deform_attn.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::vector<at::Tensor> ms_deform_attn_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/DINO/models/dino/ops/src/ms_deform_attn.h:51:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   51 |     if (\u001b[01;35m\u001b[Kvalue.type()\u001b[m\u001b[K.is_cuda())\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/DINO/models/dino/ops/src/ms_deform_attn.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/DINO/models/dino/ops/src/vision.cpp:11\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-cpython-310\n",
            "x86_64-linux-gnu-g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions build/temp.linux-x86_64-cpython-310/content/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.o build/temp.linux-x86_64-cpython-310/content/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.o build/temp.linux-x86_64-cpython-310/content/DINO/models/dino/ops/src/vision.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating MultiScaleDeformableAttention.egg-info\n",
            "writing MultiScaleDeformableAttention.egg-info/PKG-INFO\n",
            "writing dependency_links to MultiScaleDeformableAttention.egg-info/dependency_links.txt\n",
            "writing top-level names to MultiScaleDeformableAttention.egg-info/top_level.txt\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "reading manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-cpython-310/MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/MultiScaleDeformableAttention.py to MultiScaleDeformableAttention.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.MultiScaleDeformableAttention.cpython-310: module references __file__\n",
            "creating dist\n",
            "creating 'dist/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Extracting MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding MultiScaleDeformableAttention 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "Finished processing dependencies for MultiScaleDeformableAttention==1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/DINO/models/dino/ops/test.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBryoJ69X44z",
        "outputId": "91d50e57-f40d-4ced-b668-fd1eb1422e50"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* True check_forward_equal_with_pytorch_double: max_abs_err 8.67e-19 max_rel_err 2.35e-16\n",
            "* True check_forward_equal_with_pytorch_float: max_abs_err 4.66e-10 max_rel_err 1.13e-07\n",
            "* True check_gradient_numerical(D=30)\n",
            "* True check_gradient_numerical(D=32)\n",
            "* True check_gradient_numerical(D=64)\n",
            "* True check_gradient_numerical(D=71)\n",
            "* True check_gradient_numerical(D=1025)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/DINO/models/dino/ops/test.py\", line 86, in <module>\n",
            "    check_gradient_numerical(channels, True, True, True)\n",
            "  File \"/content/DINO/models/dino/ops/test.py\", line 76, in check_gradient_numerical\n",
            "    gradok = gradcheck(func, (value.double(), shapes, level_start_index, sampling_locations.double(), attention_weights.double(), im2col_step))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/gradcheck.py\", line 2052, in gradcheck\n",
            "    return _gradcheck_helper(**args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/gradcheck.py\", line 2081, in _gradcheck_helper\n",
            "    _gradcheck_real_imag(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/gradcheck.py\", line 1491, in _gradcheck_real_imag\n",
            "    gradcheck_fn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/gradcheck.py\", line 1598, in _slow_gradcheck\n",
            "    _get_numerical_jacobian(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/gradcheck.py\", line 297, in _get_numerical_jacobian\n",
            "    get_numerical_jacobian_wrt_specific_input(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/gradcheck.py\", line 488, in get_numerical_jacobian_wrt_specific_input\n",
            "    return _combine_jacobian_cols(jacobian_cols, outputs, input, input.numel())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/gradcheck.py\", line 416, in _combine_jacobian_cols\n",
            "    jacobians = _allocate_jacobians_with_outputs(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/gradcheck.py\", line 76, in _allocate_jacobians_with_outputs\n",
            "    out.append(t.new_zeros((numel_input, t.numel()), **options))\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.50 GiB. GPU 0 has a total capacity of 14.75 GiB of which 7.13 GiB is free. Process 64405 has 7.62 GiB memory in use. Of the allocated memory 7.50 GiB is allocated by PyTorch, and 541.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd DINO"
      ],
      "metadata": {
        "id": "htvwvBNvZp1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ../../.."
      ],
      "metadata": {
        "id": "3b_cmnZS-2DC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install addict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5HH5am4_Ajq",
        "outputId": "38049382-0429-40a3-dfc7-5940cacba1b7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting addict\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Installing collected packages: addict\n",
            "Successfully installed addict-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yapf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBjM6g6c_IKk",
        "outputId": "59baf617-d98f-4a6c-875c-3a69a3fefc80"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yapf\n",
            "  Downloading yapf-0.40.2-py3-none-any.whl.metadata (45 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf) (8.5.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf) (4.3.6)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf) (2.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf) (3.20.2)\n",
            "Downloading yapf-0.40.2-py3-none-any.whl (254 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/254.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yapf\n",
            "Successfully installed yapf-0.40.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/DINO\n",
        "!bash /content/DINO/scripts/DINO_train.sh /content/Pedestrian_dataset_for_internship_assignment --pretrain_model_path /content/checkpoint0011_4scale.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpNP54-oY1bN",
        "outputId": "2ed4981b-3cd8-420c-9200-31d235ad8abc"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Loading config file from config/DINO/DINO_4scale.py\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/DINO/main.py\", line 392, in <module>\n",
            "    main(args)\n",
            "  File \"/content/DINO/main.py\", line 95, in main\n",
            "    cfg = SLConfig.fromfile(args.config_file)\n",
            "  File \"/content/DINO/util/slconfig.py\", line 193, in fromfile\n",
            "    cfg_dict, cfg_text = SLConfig._file2dict(filename)\n",
            "  File \"/content/DINO/util/slconfig.py\", line 83, in _file2dict\n",
            "    check_file_exist(filename)\n",
            "  File \"/content/DINO/util/slconfig.py\", line 26, in check_file_exist\n",
            "    raise FileNotFoundError(msg_tmpl.format(filename))\n",
            "FileNotFoundError: file \"/content/config/DINO/DINO_4scale.py\" does not exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import os.path as osp\n",
        "def check_file_exist(filename, msg_tmpl='file \"{}\" does not exist'):\n",
        "    if not osp.isfile(filename):\n",
        "        raise FileNotFoundError(msg_tmpl.format(filename))\n",
        "\n",
        "\n",
        "print(check_file_exist('/content/DINO/config/DINO/DINO_4scale.py'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xArRP-TLBnxU",
        "outputId": "75184816-925b-44b7-c69a-ba843c8c2af9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yjsrwvZNRDf",
        "outputId": "3d1367f0-cc1f-4c36-d7d8-b62543e580c6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2Asn7WwNfqZ",
        "outputId": "cc417e48-3341-4a09-ff5e-b419d0355bc0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycocotools\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Cy2Uh0sNwmd",
        "outputId": "1928c6d6-7198-44e2-a925-ac5d1ac9e019"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (2.0)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools) (75.1.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.10/dist-packages (from pycocotools) (3.0.11)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n"
          ]
        }
      ]
    }
  ]
}